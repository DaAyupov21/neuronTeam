{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction\n",
    "## feature -> vae predict -> feature concat -> inverse melspectrogram -> wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import pickle\n",
    "import umap\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Conv2D, Flatten, Lambda\n",
    "from keras.layers import Reshape, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from seaborn import heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape = (129, 48, 1)\n",
    "# input_shape = (40, 48, 1)\n",
    "input_shape = (128, 48, 1)\n",
    "# intermediate_dim = 512\n",
    "intermediate_dim = 128\n",
    "latent_dim = 40\n",
    "latent_dim = 20\n",
    "# batch_size = 16\n",
    "batch_size = 3\n",
    "kernel_size = 6\n",
    "kernel_size = 3\n",
    "stride_size = 3\n",
    "stride_size = 1\n",
    "filters = 16\n",
    "filters = 4\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/seungho/anaconda3/envs/ML/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 128, 48, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 126, 46, 8)   80          encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 124, 44, 16)  1168        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 87296)        0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          11174016    flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 20)           2580        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 20)           2580        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 20)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,180,424\n",
      "Trainable params: 11,180,424\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 87296)             1833216   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 124, 44, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 126, 46, 16)       2320      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 128, 48, 8)        1160      \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 48, 1)        73        \n",
      "=================================================================\n",
      "Total params: 1,836,769\n",
      "Trainable params: 1,836,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 128, 48, 1)        0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 20), (None, 20),  11180424  \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 128, 48, 1)        1836769   \n",
      "=================================================================\n",
      "Total params: 13,017,193\n",
      "Trainable params: 13,017,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungho/anaconda3/envs/ML/lib/python3.7/site-packages/keras/engine/training_utils.py:819: UserWarning: Output decoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to decoder.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "# 모델의 아키텍쳐 정의하는 부분 중요\n",
    "# 일단 컴파일은 되는데 나중에 graphviz 에러 수정할 필요있음 이 모델을 쓸거면은\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = inputs\n",
    "for i in range(2):\n",
    "    filters *= 2\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               activation='tanh',\n",
    "               strides=stride_size,\n",
    "               padding='valid')(x)\n",
    "\n",
    "# shape info needed to build decoder model\n",
    "shape = K.int_shape(x)\n",
    "\n",
    "# generate latent vector Q(z|X)\n",
    "x = Flatten()(x)\n",
    "x = Dense(intermediate_dim, activation='tanh')(x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "# plot_model(encoder, to_file='../data/vae_cnn_encoder.png', show_shapes=True)\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(shape[1] * shape[2] * shape[3], activation='tanh')(latent_inputs)\n",
    "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "for i in range(2):\n",
    "    x = Conv2DTranspose(filters=filters,\n",
    "                        kernel_size=kernel_size,\n",
    "                        activation='tanh',\n",
    "                        strides=stride_size,\n",
    "                        padding='valid')(x)\n",
    "    filters //= 2\n",
    "\n",
    "outputs = Conv2DTranspose(filters=1,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='sigmoid',\n",
    "                          padding='same',\n",
    "                          name='decoder_output')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "# plot_model(decoder, to_file='../data/vae_cnn_decoder.png', show_shapes=True)\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
    "\n",
    "reconstruction_loss *= input_shape[0] * input_shape[1]\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -5e-4\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='rmsprop')\n",
    "# 이부분 어차피 모델 시각화 파일 저장하는거라 필요할때 에러 처리하도록\n",
    "# plot_model(vae, to_file='../data/vae_cnn.png', show_shapes=True)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 아키텍쳐 맨날 했던부분\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = inputs\n",
    "for i in range(2):\n",
    "    filters *= 2\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               activation='tanh',\n",
    "               strides=3,\n",
    "               padding='valid')(x)\n",
    "\n",
    "# shape info needed to build decoder model\n",
    "shape = K.int_shape(x)\n",
    "\n",
    "# generate latent vector Q(z|X)\n",
    "x = Flatten()(x)\n",
    "x = Dense(intermediate_dim, activation='tanh')(x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "# plot_model(encoder, to_file='../data/vae_cnn_encoder.png', show_shapes=True)\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(shape[1] * shape[2] * shape[3], activation='tanh')(latent_inputs)\n",
    "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "for i in range(2):\n",
    "    x = Conv2DTranspose(filters=filters,\n",
    "                        kernel_size=kernel_size,\n",
    "                        activation='tanh',\n",
    "                        strides=3,\n",
    "                        padding='valid')(x)\n",
    "    filters //= 2\n",
    "\n",
    "outputs = Conv2DTranspose(filters=1,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='sigmoid',\n",
    "                          padding='same',\n",
    "                          name='decoder_output')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "# plot_model(decoder, to_file='../data/vae_cnn_decoder.png', show_shapes=True)\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "# plot_model(vae, to_file='../data/vae_cnn.png', show_shapes=True)\n",
    "\n",
    "reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
    "\n",
    "reconstruction_loss *= input_shape[0] * input_shape[1]\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -5e-4\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='rmsprop')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model weight\n",
    "vae.load_weights('../son/model/99.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy pickle load error 해결\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "x_mean = pickle.load(open('../data/x_mean.pkl', 'rb'))\n",
    "x_std = pickle.load(open('../data/x_std.pkl', 'rb'))\n",
    "\n",
    "# 이부분 까지는 원래부터 계속 했던거 모델 설계, 웨이트, mean, std 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature 가져와서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 128, 48) 163-122947-0000\n",
      "predict (6, 128, 48, 1)\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "feature_path = '../son/feature_train/'\n",
    "recon_path = '../son/reconstruction'\n",
    "for i, filename in enumerate(sorted(os.listdir(feature_path))):\n",
    "    \n",
    "    # 특정 파일만 확인\n",
    "    # x = (np.load('../son/feature_train/NB10584578') - x_mean) / x_std\n",
    "    x = (np.load(os.path.join(best_test, filename)) - x_mean) / x_std\n",
    "    print(x.shape, filename)  \n",
    "    \n",
    "    # batch 사이즈만큼 학습했기 때문에 학습한 batch size만큼 잘라줘서 predict 해주기\n",
    "    split_column = int(x.shape[0]/batch_size) * batch_size\n",
    "    x = x[:split_column, :, :]\n",
    "    \n",
    "    # predict\n",
    "    predict = vae.predict(x.reshape(x.shape + (1,)))\n",
    "    print(\"predict\", predict.shape)\n",
    "    predict = predict.reshape(x.shape)\n",
    "    \n",
    "    # feature concat해서 inverse한다음에 wav파일로 저장 \n",
    "    last = librosa.feature.inverse.mel_to_audio(np.hstack(predict))\n",
    "    librosa.output.write_wav(os.path.join(recon_path, filename), last, sr=22050)                    \n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== 밑에는 이전코드 시각화 하고... 하는거 일단 패스 ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best test는 또 뭐지\n",
    "# best_test = '/home/ds/DataScience/Datasets/LibriSpeech/VAELibriSpeech/best_test'\n",
    "best_test = '../son/feature_test/'\n",
    "all_x = []\n",
    "all_x_pred = []\n",
    "all_y = []\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "# best_test가먼데에에에 결국 따로 파일 만들고 테스트 해봐야하나\n",
    "for i, filename in enumerate(sorted(os.listdir(best_test))):\n",
    "    \n",
    "#     x, sample_rate = librosa.load(best_test + filename, sr=16000\n",
    "#     x = (x - x_mean) / x_std\n",
    "    \n",
    "    x = (np.load(os.path.join(best_test, filename)) - x_mean) / x_std\n",
    "    print(x.shape)\n",
    "    x_ing = x[:len(x) - (len(x) % 48)] # 48 쉐이프 맞춰주기위해\n",
    "    x_ing = np.reshape(x_ing, (-1, 48))\n",
    "    print(x_ing.shape)\n",
    "    x_ing = x_ing[:40, :]\n",
    "    print(x_ing.shape)\n",
    "        \n",
    "    # x가 전체 데이터고 ing이 하나만 딱 뽑는 거인듯!!! 정신차려~~\n",
    "#     x_ing = x[np.random.choice(x.shape[0]), :, :]\n",
    "\n",
    "    # 전체 x랑 y 저장 y는 categorical 만들어주기 위해 저렇게 한건가\n",
    "    all_x += [x]\n",
    "    all_y += [i]*x.shape[0]\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(3, 7))\n",
    "    # 랜덤 셔플한거의 dim2 쉐이프랑 이미지 보여주기\n",
    "#     print(x_ing.shape[1]) # range(0, 48) shape[1] = 48 shape[0] = \n",
    "    \n",
    "    plt.pcolormesh(range(x_ing.shape[1]),\n",
    "                   range(x_ing.shape[0]),\n",
    "                   10*np.log10(x_ing))\n",
    "    plt.savefig('../data/{}_true.png'.format(filename))\n",
    "    plt.show()\n",
    "    \n",
    "    # predict부분 reshape를 해서 predict한다음 다시 원래 위치로 컴백\n",
    "    x_pred = vae.predict(x_ing.reshape((1,\n",
    "                                        x_ing.shape[0],\n",
    "                                        x_ing.shape[1], 1))).reshape((x_ing.shape[0],\n",
    "                                                                      x_ing.shape[1]))\n",
    "    # 예측한 것 = reconstruction 그림으로 보여주기\n",
    "    print(x_pred)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(librosa.power_to_db(x_pred, ref=np.max), y_axis='mel', x_axis='time')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Reconstruct Mel-Spectrogram')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Mel-Spectrogram example.png')\n",
    "    plt.show()\n",
    "\n",
    "#     all_x_pred += [x_pred]\n",
    "#     plt.figure(figsize=(3, 7))\n",
    "#     plt.pcolormesh(range(x_pred.shape[1]),\n",
    "#                    range(x_pred.shape[0]),\n",
    "#                    10*np.log10(x_pred))\n",
    "#     plt.savefig('../data/{}_reconstr.png'.format(filename))\n",
    "#     plt.show()\n",
    "    \n",
    "# 전부다 합치기...\n",
    "all_x = np.vstack(all_x)\n",
    "all_y = np.stack(all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일단 인풋데이터의 형식이 (129, 48, 1)\n",
    "# 이건 피쳐임\n",
    "# value\n",
    "# \n",
    "all_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === 여기는 각 화자가 비슷한 벡터로 임베딩 되었는지 확인하는 부분 ===\n",
    "# 다른사람 목소리 들어오면 여기다 해주면 될듯!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 부분이 테스트 할려고 각 사람의 아이디를 가져와서 하는 부분인가봐 \n",
    "all_x = []\n",
    "all_y = []\n",
    "\n",
    "# speaker_ids = [\"1089\", \"1188\", \"121\", \"1221\", \"1284\", \"1320\", \"1580\",\n",
    "#                \"1995\", \"2094\", \"2300\", \"237\", \"260\", \"2830\", \"2961\",]\n",
    "# other_ids = [\"3570\", \"3575\", \"3729\", \"4077\", \"4446\", \"4507\", \"4970\",\n",
    "#                \"4992\", \"5105\", \"5142\", \"5639\", \"5683\", \"61\", \"672\",\n",
    "#                \"6829\", \"6930\", \"7021\", \"7127\", \"7176\", \"7729\", \"8224\",\n",
    "#                \"8230\", \"8455\", \"8463\", \"8555\", \"908\"]\n",
    "\n",
    "speaker_ids = ['1', '19', '118', '32']\n",
    "\n",
    "\n",
    "# 피쳐 경로\n",
    "# feats_path = '/home/ds/DataScience/Datasets/LibriSpeech/VAELibriSpeech/test-clean-wav/'\n",
    "feats_path = '../son/feature_test/'\n",
    "\n",
    "# id 별로 encode하기 \n",
    "for filename in sorted(os.listdir(feats_path)):\n",
    "    cur_speaker_id = filename.split('-')[0]\n",
    "    if cur_speaker_id == 'NB':\n",
    "        cur_speaker_id = '1'\n",
    "    all_y += [cur_speaker_id]\n",
    "    x_file = np.load(os.path.join(feats_path, filename))\n",
    "    x_file = (x_file - x_mean) / x_std\n",
    "    all_x_encoded = encoder.predict(x_file.reshape(x_file.shape + (1,)))[2]\n",
    "    all_x += [np.max(all_x_encoded, axis=0)]\n",
    "all_x = np.stack(all_x)\n",
    "all_y = np.stack(all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator로 만들기\n",
    "ids2labels = {speaker_id: i for i, speaker_id in enumerate(speaker_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embedded.shape, all_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate into the latent space\n",
    "\n",
    "# encode된 latent vector를 차원축소 해서 뿌려보기\n",
    "# id 대부분이 겹쳐 있는것을 볼 수 있음\n",
    "x_embedded = decomposition.fit_transform(all_x)\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(x_embedded[:, 0], x_embedded[:, 1], c=[int(ids2labels[speaker_id])*30 for speaker_id in all_y])\n",
    "\n",
    "# annotate!\n",
    "for i, txt in enumerate(all_y):\n",
    "    ax.annotate(txt, (x_embedded[i, 0], x_embedded[i, 1]))\n",
    "#plt.colorbar()\n",
    "plt.savefig('../data/speakers_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 부분운 단어들 가지고 하는거인듯 위에꺼랑 다른 거인 듯\n",
    "all_x = []\n",
    "all_y = []\n",
    "feats_path = '/home/ds/DataScience/Datasets/LibriSpeech/VAELibriSpeech/test-words-feats/'\n",
    "for filename in sorted(os.listdir(feats_path)):\n",
    "    x_file = np.load(os.path.join(feats_path, filename))\n",
    "    x_file = (x_file - x_mean) / x_std\n",
    "    all_x_encoded = encoder.predict(x_file.reshape(x_file.shape + (1,)))[2]\n",
    "    all_x += [np.max(all_x_encoded, axis=0)]\n",
    "    all_y += [filename]\n",
    "all_x = np.stack(all_x)\n",
    "all_y = np.stack(all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate into the latent space\n",
    "x_embedded = decomposition.fit_transform(all_x)\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(x_embedded[:, 0], x_embedded[:, 1])\n",
    "\n",
    "for i, txt in enumerate(all_y):\n",
    "    ax.annotate(txt, (x_embedded[i, 0], x_embedded[i, 1]))\n",
    "#plt.colorbar()\n",
    "plt.savefig('../data/words_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embedded.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
