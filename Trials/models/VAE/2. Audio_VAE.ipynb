{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Audio_VAE.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"e-b8vHgJ1MNV","colab_type":"text"},"source":["# feature 데이터를 가져와서 vae 모델 학습시키고 mean, std, model weight 저장"]},{"cell_type":"code","metadata":{"id":"IqLuB92P1MNY","colab_type":"code","colab":{}},"source":["%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gZUl0y_x1MNg","colab_type":"code","colab":{}},"source":["import os\n","import keras\n","import pickle\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from keras.layers import Dense, Input\n","from keras.layers import Conv2D, Flatten, Lambda\n","from keras.layers import Reshape, Conv2DTranspose\n","from keras.models import Model\n","from keras.losses import mse, binary_crossentropy\n","from keras.utils import plot_model\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras import backend as K"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R6-wrIri1MNn","colab_type":"code","colab":{}},"source":["os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bUZdCWmV1MNu","colab_type":"code","colab":{}},"source":["# input_shape = (129, 48, 1)\n","# input_shape = (40, 48, 1)\n","input_shape = (128, 48, 1)\n","# intermediate_dim = 512\n","intermediate_dim = 128\n","latent_dim = 40\n","latent_dim = 20\n","# batch_size = 16\n","batch_size = 3\n","kernel_size = 6\n","kernel_size = 3\n","stride_size = 3\n","stride_size = 1\n","filters = 16\n","filters = 4\n","epochs = 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pr4obfZ71MN3","colab_type":"code","colab":{},"outputId":"e64b3fef-74bf-40ca-894e-9aceaf6b1f2f"},"source":["# 모델의 아키텍쳐 정의하는 부분 중요\n","# 일단 컴파일은 되는데 나중에 graphviz 에러 수정할 필요있음 이 모델을 쓸거면은\n","def sampling(args):\n","    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n","    # Arguments\n","        args (tensor): mean and log of variance of Q(z|X)\n","    # Returns\n","        z (tensor): sampled latent vector\n","    \"\"\"\n","\n","    z_mean, z_log_var = args\n","    batch = K.shape(z_mean)[0]\n","    dim = K.int_shape(z_mean)[1]\n","    # by default, random_normal has mean=0 and std=1.0\n","    epsilon = K.random_normal(shape=(batch, dim))\n","    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n","\n","# VAE model = encoder + decoder\n","# build encoder model\n","inputs = Input(shape=input_shape, name='encoder_input')\n","x = inputs\n","for i in range(2):\n","    filters *= 2\n","    x = Conv2D(filters=filters,\n","               kernel_size=kernel_size,\n","               activation='tanh',\n","               strides=stride_size,\n","               padding='valid')(x)\n","\n","# shape info needed to build decoder model\n","shape = K.int_shape(x)\n","\n","# generate latent vector Q(z|X)\n","x = Flatten()(x)\n","x = Dense(intermediate_dim, activation='tanh')(x)\n","z_mean = Dense(latent_dim, name='z_mean')(x)\n","z_log_var = Dense(latent_dim, name='z_log_var')(x)\n","\n","# use reparameterization trick to push the sampling out as input\n","# note that \"output_shape\" isn't necessary with the TensorFlow backend\n","z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n","\n","# instantiate encoder model\n","encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n","encoder.summary()\n","# plot_model(encoder, to_file='../data/vae_cnn_encoder.png', show_shapes=True)\n","\n","# build decoder model\n","latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n","x = Dense(shape[1] * shape[2] * shape[3], activation='tanh')(latent_inputs)\n","x = Reshape((shape[1], shape[2], shape[3]))(x)\n","\n","for i in range(2):\n","    x = Conv2DTranspose(filters=filters,\n","                        kernel_size=kernel_size,\n","                        activation='tanh',\n","                        strides=stride_size,\n","                        padding='valid')(x)\n","    filters //= 2\n","\n","outputs = Conv2DTranspose(filters=1,\n","                          kernel_size=kernel_size,\n","                          activation='sigmoid',\n","                          padding='same',\n","                          name='decoder_output')(x)\n","\n","# instantiate decoder model\n","decoder = Model(latent_inputs, outputs, name='decoder')\n","decoder.summary()\n","# plot_model(decoder, to_file='../data/vae_cnn_decoder.png', show_shapes=True)\n","\n","# instantiate VAE model\n","outputs = decoder(encoder(inputs)[2])\n","vae = Model(inputs, outputs, name='vae')\n","\n","reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n","\n","reconstruction_loss *= input_shape[0] * input_shape[1]\n","kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n","kl_loss = K.sum(kl_loss, axis=-1)\n","kl_loss *= -5e-4\n","vae_loss = K.mean(reconstruction_loss + kl_loss)\n","vae.add_loss(vae_loss)\n","vae.compile(optimizer='rmsprop')\n","# 이부분 어차피 모델 시각화 파일 저장하는거라 필요할때 에러 처리하도록\n","# plot_model(vae, to_file='../data/vae_cnn.png', show_shapes=True)\n","vae.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /home/seungho/anaconda3/envs/ML/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Model: \"encoder\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","encoder_input (InputLayer)      (None, 128, 48, 1)   0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 126, 46, 8)   80          encoder_input[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 124, 44, 16)  1168        conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 87296)        0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 128)          11174016    flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","z_mean (Dense)                  (None, 20)           2580        dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","z_log_var (Dense)               (None, 20)           2580        dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","z (Lambda)                      (None, 20)           0           z_mean[0][0]                     \n","                                                                 z_log_var[0][0]                  \n","==================================================================================================\n","Total params: 11,180,424\n","Trainable params: 11,180,424\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Model: \"decoder\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","z_sampling (InputLayer)      (None, 20)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 87296)             1833216   \n","_________________________________________________________________\n","reshape_1 (Reshape)          (None, 124, 44, 16)       0         \n","_________________________________________________________________\n","conv2d_transpose_1 (Conv2DTr (None, 126, 46, 16)       2320      \n","_________________________________________________________________\n","conv2d_transpose_2 (Conv2DTr (None, 128, 48, 8)        1160      \n","_________________________________________________________________\n","decoder_output (Conv2DTransp (None, 128, 48, 1)        73        \n","=================================================================\n","Total params: 1,836,769\n","Trainable params: 1,836,769\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"vae\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","encoder_input (InputLayer)   (None, 128, 48, 1)        0         \n","_________________________________________________________________\n","encoder (Model)              [(None, 20), (None, 20),  11180424  \n","_________________________________________________________________\n","decoder (Model)              (None, 128, 48, 1)        1836769   \n","=================================================================\n","Total params: 13,017,193\n","Trainable params: 13,017,193\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/home/seungho/anaconda3/envs/ML/lib/python3.7/site-packages/keras/engine/training_utils.py:819: UserWarning: Output decoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to decoder.\n","  'be expecting any data to be passed to {0}.'.format(name))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"y2YfyALI1MN_","colab_type":"text"},"source":["# Finding approximate mean and std of data: mean이랑 std 찾기"]},{"cell_type":"code","metadata":{"id":"9-bz_y4-1MOB","colab_type":"code","colab":{},"outputId":"fcd20f27-d74a-48f0-a187-f2065f6ed34f"},"source":["%%time\n","\n","# import numpy as np\n","# numpy pickle load error 해결\n","np_load_old = np.load\n","\n","# modify the default parameters of np.load\n","np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n","\n","x_train = []\n","\n","# 트레인 피쳐 가져와서\n","train_features_path = '../son/feature_train/'\n","n_files = len(os.listdir(train_features_path))\n","n_train = 0\n","for filename in sorted(os.listdir(train_features_path)):\n","    full_filename = os.path.join(train_features_path, filename)\n","    print(full_filename)\n","    data = np.load(full_filename)\n","    n_train += data.shape[0]\n","    # 데이터 23개\n","    x_train += [data[np.random.randint(data.shape[0], size=1)]]\n","\n","# feature 데이터 다 가져와서 x_train에 다 합치는 거 같은데 어떻게 하는지는 자세히 모르겠네...\n","# 왜 랜덤으로 하는거지\n","print(len(x_train))\n","x_train = np.vstack(x_train)\n","x_mean = np.mean(x_train)\n","x_std = np.std(x_train)\n","\n","print(x_train.shape, n_train)\n","print('mean', x_mean, 'std', x_std)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["../son/feature_train/163-122947-0000\n","../son/feature_train/163-122947-0001\n","../son/feature_train/163-122947-0002\n","../son/feature_train/163-122947-0003\n","../son/feature_train/163-122947-0004\n","../son/feature_train/163-122947-0005\n","../son/feature_train/163-122947-0006\n","../son/feature_train/163-122947-0007\n","../son/feature_train/163-122947-0008\n","../son/feature_train/163-122947-0009\n","../son/feature_train/163-122947-0010\n","../son/feature_train/163-122947-0011\n","../son/feature_train/163-122947-0012\n","../son/feature_train/163-122947-0013\n","../son/feature_train/163-122947-0014\n","../son/feature_train/163-122947-0015\n","../son/feature_train/163-122947-0016\n","../son/feature_train/163-122947-0017\n","../son/feature_train/163-122947-0018\n","../son/feature_train/163-122947-0019\n","../son/feature_train/163-122947-0020\n","../son/feature_train/163-122947-0021\n","../son/feature_train/163-122947-0022\n","../son/feature_train/19-198-0000\n","../son/feature_train/19-198-0001\n","../son/feature_train/19-198-0002\n","../son/feature_train/19-198-0003\n","../son/feature_train/19-198-0004\n","../son/feature_train/19-198-0005\n","../son/feature_train/19-198-0006\n","../son/feature_train/19-198-0007\n","../son/feature_train/19-198-0008\n","../son/feature_train/19-198-0009\n","../son/feature_train/19-198-0010\n","../son/feature_train/19-198-0011\n","../son/feature_train/19-198-0012\n","../son/feature_train/19-198-0013\n","../son/feature_train/19-198-0014\n","../son/feature_train/19-198-0015\n","../son/feature_train/19-198-0016\n","../son/feature_train/19-198-0017\n","../son/feature_train/19-198-0018\n","../son/feature_train/19-198-0019\n","../son/feature_train/19-198-0020\n","../son/feature_train/19-198-0027\n","../son/feature_train/19-198-0028\n","../son/feature_train/32-21631-0000\n","../son/feature_train/32-21631-0001\n","../son/feature_train/32-21631-0002\n","../son/feature_train/32-21631-0003\n","../son/feature_train/32-21631-0004\n","../son/feature_train/32-21631-0005\n","../son/feature_train/32-21631-0006\n","../son/feature_train/32-21631-0007\n","../son/feature_train/32-21631-0008\n","../son/feature_train/32-21631-0009\n","../son/feature_train/32-21631-0010\n","../son/feature_train/32-21631-0011\n","../son/feature_train/32-21631-0012\n","../son/feature_train/32-21631-0013\n","../son/feature_train/32-21631-0014\n","../son/feature_train/32-21631-0015\n","../son/feature_train/32-21631-0016\n","../son/feature_train/32-21631-0017\n","../son/feature_train/32-21631-0018\n","../son/feature_train/32-21631-0019\n","../son/feature_train/NB10584578\n","../son/feature_train/NB10585784\n","../son/feature_train/NB10587175\n","../son/feature_train/NB10588435\n","../son/feature_train/NB10591910\n","../son/feature_train/NB10593181\n","../son/feature_train/NB10594493\n","../son/feature_train/NB10595786\n","../son/feature_train/NB10598644\n","../son/feature_train/NB10599787\n","../son/feature_train/NB10600918\n","../son/feature_train/NB10601681\n","../son/feature_train/NB10604725\n","../son/feature_train/NB10605984\n","../son/feature_train/NB10607174\n","../son/feature_train/NB10611681\n","../son/feature_train/NB10612821\n","../son/feature_train/NB10613998\n","../son/feature_train/NB10615428\n","../son/feature_train/NB10618854\n","../son/feature_train/NB10620453\n","../son/feature_train/NB10622041\n","../son/feature_train/NB10623646\n","89\n","(89, 128, 48) 885\n","mean 0.5478858 std 6.1631556\n","CPU times: user 120 ms, sys: 7.5 ms, total: 128 ms\n","Wall time: 117 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kn1Q17xU1MOL","colab_type":"code","colab":{},"outputId":"fb3eafb6-1ea0-4683-a86a-dc5c77d643d3"},"source":["# 위에거를 테스트로 해보면 이렇게 됨 이거는 개수만 세는거라 큰 의미가 없음!\n","# test_features_path = '/home/ds/DataScience/Datasets/LibriSpeech/VAELibriSpeech/test-clean-wav/'\n","test_features_path = '../son/feature_test/'\n","n_files = len(os.listdir(test_features_path))\n","n_test = 0\n","for filename in sorted(os.listdir(test_features_path)):\n","    full_filename = os.path.join(test_features_path, filename)\n","    print(full_filename)\n","    data = np.load(full_filename)\n","    n_test += data.shape[0]\n","    \n","# (n_train, n_test) (1047736, 30548)\n","(n_train, n_test) # (1456, 413)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["../son/feature_test/118-47824-0079\n","../son/feature_test/118-47824-0080\n","../son/feature_test/118-47824-0081\n","../son/feature_test/118-47824-0082\n","../son/feature_test/118-47824-0083\n","../son/feature_test/118-47824-0084\n","../son/feature_test/118-47824-0085\n","../son/feature_test/118-47824-0086\n","../son/feature_test/19-198-0031\n","../son/feature_test/19-198-0032\n","../son/feature_test/19-198-0033\n","../son/feature_test/19-198-0034\n","../son/feature_test/19-198-0035\n","../son/feature_test/19-198-0036\n","../son/feature_test/19-198-0037\n","../son/feature_test/32-21631-0014\n","../son/feature_test/32-21631-0015\n","../son/feature_test/32-21631-0016\n","../son/feature_test/32-21631-0017\n","../son/feature_test/32-21631-0018\n","../son/feature_test/32-21631-0019\n","../son/feature_test/NB10627990\n","../son/feature_test/NB10629447\n","../son/feature_test/NB10631198\n","../son/feature_test/NB10633045\n","../son/feature_test/NB10637509\n","../son/feature_test/NB10639096\n","../son/feature_test/NB10640807\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(885, 254)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"DO09SgoC1MOU","colab_type":"code","colab":{}},"source":["# 여기서 저장 하는거!\n","pickle.dump(x_mean, open('../data/x_mean.pkl', 'wb'))\n","pickle.dump(x_std, open('../data/x_std.pkl', 'wb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AWP_iSJz1MOb","colab_type":"code","colab":{}},"source":["# 여기서 불러오는거 이거를 결국 vae 모델 weight 만들 때 씀\n","x_mean = pickle.load(open('../data/x_mean.pkl', 'rb'))\n","x_std = pickle.load(open('../data/x_std.pkl', 'rb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p7Dlni6Q1MOg","colab_type":"code","colab":{},"outputId":"222326fa-62be-43bd-8b15-5ec3352977bf"},"source":["%%time\n","x_train = []\n","\n","n_files = len(os.listdir(train_features_path))\n","for epoch in range(epochs):\n","    print(epoch)\n","    for filename in sorted(os.listdir(train_features_path)):\n","#         print(filename)\n","        full_filename = os.path.join(train_features_path, filename)\n","        data = np.load(full_filename)\n","        # train 데이터 정규화\n","        x_train = (data - x_mean)/x_std\n","        # 그냥 데이터 1차원 추가 input형식 맞추주기 위해\n","        x_train = x_train.reshape(x_train.shape + (1,))\n","\n","        # batch 사이즈로 잘라주기 \n","        n_batches = int(data.shape[0] / batch_size)\n","\n","        # np array를 배치 = 3 로 자르기 \n","        for batch in np.array_split(x_train, [ind*batch_size for ind in range(1, n_batches+1)]):\n","            # 배치 모양 정확하지 않으면 실행하지 않음\n","            if batch.shape != (batch_size, data.shape[1], data.shape[2], 1):\n","                continue\n","            # train on batch라는 함수도 있음\n","            batch_loss = vae.train_on_batch(batch, y=None)\n","#         print('filename:', filename, 'loss:', batch_loss)\n","    # 여기서 웨이트 저장\n","    vae.save_weights('../son/model/{}.h5'.format(epoch))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","WARNING:tensorflow:From /home/seungho/anaconda3/envs/ML/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","CPU times: user 5h 40min 43s, sys: 45min 19s, total: 6h 26min 2s\n","Wall time: 1h 2min 9s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kJMk2Jqo1MOp","colab_type":"text"},"source":["# ==== 밑에는 배치를 안하고 돌리는거 ! 안해도 됨!"]},{"cell_type":"code","metadata":{"id":"-Wa3LEnu1MOr","colab_type":"code","colab":{}},"source":["%%time\n","# 여기는 확인하는 코드인듯 위에 부분이랑 배치해주는 부분만 다름\n","x_train = []\n","train_features_path = '/home/ds/DataScience/Datasets/LibriSpeech/VAELibriSpeech/train-clean-wav/'\n","n_files = len(os.listdir(train_features_path))\n","for epoch in range(epochs):\n","    print(epoch)\n","    for filename in sorted(os.listdir(train_features_path)):\n","        print(filename)\n","        full_filename = os.path.join(train_features_path, filename)\n","        data = np.load(full_filename)\n","        data = data[np.random.randint(data.shape[0], size=batch_size), :, :]\n","        x_train = (data - x_mean)/x_std\n","        x_train = x_train.reshape(x_train.shape + (1,))\n","        print(vae.train_on_batch(x_train, y=None))\n","    vae.save_weights('/home/ds/DataScience/Models/audio_vae/10/vae_cnn_audio_{}.h5'.format(epoch))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IIh7782z1MOw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZnVhzdgU1MO1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}