# Abstract

보이스 컨버젼에 딥러닝 사용<br>
지도학습으로 하지않음<br>
cnn encoder를 사용함 - wavenet decoder, ?? classfier<br>
각각의 singer는 embedding vector로 표현됨<br>

상대적으로 데이터셋이 적어 새로운 데이터 증가 방법을 고안<br>
back translation을 기반한 새로운 트레이닝 loss와 규약들

평가는 목적으로하는 가수와 얼마나 유사한지

---

# Introduction
사람의 목소리는 가장 중요한 악기<br>
최근에 노래하는 목소리를 합성하는것이 성공적으로 적용<br>
voice -> anoter singer voice<br>
오토튠 같은 소프트웨어로도 피치조정은 가능하지만 우리는 다른 보이스 특징들을 따라 유연성을 제공함<br>
unsupervised로함<br>
parallel training을 진행하지 않음<br>
샘플로 존재하는 것보다 훨씬 더 간단한 방법 제공<br>
음성분리기술은 이미 많이 발전해옴<br>
기술적 관점에서 bactranslation, mixup 새로운 기술들을 사용<br>
변환하기위해 양쪽 voice를 5분에서 30분정도 학습함<br>
> 1. unsupervised로 하기 위해 target singer는 다른 노래로 modeld 되어있어야함
2. single encoder / 비지도학습인 조건부 decoder 효율성 입증
3. 비지도 변환 안에 두 구문 훈련 접근 법
4. backtranlation 소개
5. 데이터의 새로운 증가방법 제안

---

# 2. Related Work
wavenet autoencoder에 기초되어있음<br>
원래 autoencoder는 악기를 model하기위해 씀<br>
single encoder와 multiple decoder 를 씀으로 음악분야에서 많은 변환에 쓰임<br>
parallel데이터가 없어도 진행 됨<br>
wavenet은 다른 데이터 증가 방법을 씀<br>
최근 연구의 대부분은 wavenet decoder는 supervised를 함<br>

그 비지도 학습 vq-vae방법에서 voice conversion은<br>
양자화된 잠재적 공간인 wavenet autoencoder를 포함함<br>
decoder는 target 목소리의 특성에 영향을 미침 원핫 인코딩을 통하여
이산화 되어 bottleneck 효과 때문에 아마 불변한 스피커의 임베딩으로 이끌 것이다.<br>
domain confusion loss를 사용함 -> 이산데이터에서 월등히 작동<br>

다른 autoencoder는 스펙트럼 프레임을 만들기위해 다양한 autoencoder를 사용함<br>
10번 논문에서 single encoder, parameter decoder가 설명됨<br>
그 후에 이 방법은 WGAN을 포함하여 자연스러움을 증가시키기 위해 사용됨<br>

## single synthesis and conversion 노래 합성 및 변환
고전 방법은 concat 하거나 hmm 기반에 둔 방법을 씀<br>
wavenet decoder를 사용한 노래합성이 괜찮음을 증명<br>
이것은 음과 가사를 인풋으로 받아 vocoder 특징들의 흐름을 만듬<br>
지도학습으로 구성되었고 깨끗한 데이터에 기초하여 생성됨

여태까지 singing voice conversion은 대부분 parallel 데이터 사용<br>
학습할려면 같은 노래로 부른 사람이 필요<br>
기존 방법 중에서 결과를 비교하는 코드나 성능지표 제공 x<br>
최근에는 타겟싱어의 목소리로부터 추출된 어쿠스틱 특징들로 parallel데이터를 사용하지 않게 되었다.<br>
Vocoder Feature 4명의 가수들로부터 증명 됨.<br>

## backtranlation
자연어처리에서 등장 - 자동번역기에서 단일 말뭉치를 실행하는 테크닉
A: 번역할 쌍이 없음
B: A를 기계 번역한 것
B를 다시 A로 번역
변환시스템이 symmetric해서 backtranlation을 활용할 수 있음
네트워크에 제공되는것에 한계가 있어 다른 사람들보다
원본 가수와 가장 유사한 가상 특징들을 사용하며 생성된다.

## mixup training
두 개의 샘플을 합친것으로 훈련한다.<br>
(x1, y1), (x2, y2) 둘다 랜덤 가중치 가짐<br>
x0= 베타x1 + (1 - 베타)x2 (beta distibution)<br>
베타 분포의 shape파리미터는 0.2로 취하여 가장자리 중 하나에 가까운 샘플링된 값<br>
즉 베타가 종종 0에 가깝거나 1에 가까운 값을 생성한다<br>
> 1. mixed audio의 생성이 아니라 mixed identity를 생성
> 2. 샘플끼리 관련이 없어야 하므로 베타 분포는 고정한다.

---

# 3 - method
노래 변환은 single encoder와 가수 디코더로 이루어지고<br>
목적 가수의 벡터 임베딩의 조건화 된다.<br>
- training의 두가지 방법
>1. softmax기반 각각의 가수에게 loss를 재정립
>2. 새로운 가수들은 기존 훈련된 가수들의 벡터 값을 섞어 사용
   네트워크는 새로운 샘플들을 만들기 위한 훈련 샘플들로 변환하기 위해 훈련

## 3-1 conversion network
샘플은 오리지널 샘플 or augumentation 샘플<br>
D[u] = wavenet 디코데에 vector u<br>
C = 가수 분류 네트워크<br>
u = 각 가수의 학습되는 임베딩 벡터<br>
A = Look Up Table u를 저장함<br>
각 벡터가 norm이 1.0보다 크면 normalize 해준다.<br>
C는 sample(인코더에의해 생성된 잠재적 vector)과 가장 유사한 가수를 예측함.<br>
그림에 대한 loss 수식 설명<br>
decoder는 auto regreessive model<br>
input은 가수의 임베딩 벡터<br>
생성된 출력 대신에 이전 시간 단계에서 sj를 공급받는다.<br>

2번째 사진까지의 훈련과정을 가진 네트워크는 본래 신호를 만들 수 있다.<br>
encoder는 가수 벡터를 생성한다.<br>
가수 변환을 직접적으로 훈련하지 않는다.<br>

두번째 단계는 parallel 샘플을 생성하고 학습하기위해 backtranlation이 적용된다. <br>
이것은 mixup 기술의 결합으로 행해진다.<br>
두명의 다른 가수들의 임베딩으로 만들어진다.<br>
알파는 정규분포를 따르고 mixup샘플은 현재네트워크에서 변현중인동안 생성됨<br>

## 3.2 Audio Input augmentation
모든 샘플은 8bit mu-law encoding을 따름<br>
오디오 품질은 안좋지만 효율적인 훈련을 위해<br>
encoder가 의미있는 정보를 가지기 위해 입력 오디오의 음을 locally하게 바꿨다.<br>
이 augmentation은 별로 도움이 되지 않았다.<br>
짧은 노래들의 데이터 셋 문제가 너무 컸다.<br>
신호가 거꾸로 재생될 때 에너지 스펙트럼이 변하지 않는 사실을 활용<br>
오디오는 압력 진폭의 변화로 나타내는 사실도 의존<br>
사람은 시그널의 단계에 영향을 받지 않는다. 따라서  180도의 위상변화를 이룰 수 있다.<br>
데이터 4배 증가를 이루어냄<br>
똑바로 거꾸로 둘다 시행하고<br>
raw audio signal의 값들의 -1 곱<br>
거꾸로한걸 뒤집어서 한번<br>
첫번째는 같은 가수로 식별 불가능한 이상한 노래를 만들고<br>
두번째는 인식할 수 없지만 새로운 신호를 만듬<br>

## 3.3 the architecture of the sub-networks
그 autoencoder 네트워크는 wavenet으로 구성되는데<br>
decoder D + 확장된 convolutuon encoder E<br>
decoder는 가수의 임베딩 벡터와 encoder에 의해 생성된 잠재적표현에 영향을 받는다.<br>
encoder, decoder, confusion network는 autoencoder에서도 대부분 재사용함<br>
encdoer E는 fully convolutional netowrk이다.<br>
열개의 residual layer network의 3block = total 30layer을 포함한다.<br>
각각은 relu 쓰고 128채널 고정<br>
average pooling layer,  kernel size = 50ms<br>
12.5의 down sampling 효과를 가짐<br>

오디오 인코딩은 목적 가수 임베딩과 결합됨 -> 128차원의 벡터<br>
이 벡터의 반은 시간에 따라 변하고 나머지는 변하지 않는다.<br>
인코딩은 임시적으로 본래 audio rate로 upsampled 된다.<br>
조건화된 신호는 1x1 층을 통과하여 wavenet decoder에 여러번 보내진다.<br>
wavenet decoder는 4 x 10 rasidual layer들을 가진다.<br>
250ms의 수용 지역으로 이끈다.<br>

elu
wavenet
