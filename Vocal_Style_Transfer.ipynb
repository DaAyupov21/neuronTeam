{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Vocal_Style_Transfer.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"W5GNt5ReBuv0"},"source":["# Drive Mount & Configuration"]},{"cell_type":"code","metadata":{"id":"n7gDl5Y8Bd_Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605676400909,"user_tz":-180,"elapsed":19837,"user":{"displayName":"Aigul Sibgatullina","photoUrl":"","userId":"02965597584056384610"}},"outputId":"8d08d233-2e48-42d7-ec4d-dd1d806eb68c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9Olw2M8_B5-4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605676447233,"user_tz":-180,"elapsed":19581,"user":{"displayName":"Aigul Sibgatullina","photoUrl":"","userId":"02965597584056384610"}},"outputId":"2030456b-4c33-46fc-823d-ae5f6e0cc5f7"},"source":["# Library installation\n","!pip install pyworld==0.2.4"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting pyworld==0.2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/bf/611107c139aba3af4de0e307d4e9285fbfd19411aaf91ce16a7ece62b7fe/pyworld-0.2.4.tar.gz (72kB)\n","\r\u001b[K     |████▌                           | 10kB 15.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 20kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 30kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 40kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 61kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 71kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyworld==0.2.4) (1.18.5)\n","Requirement already satisfied: cython>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from pyworld==0.2.4) (0.29.21)\n","Building wheels for collected packages: pyworld\n","  Building wheel for pyworld (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyworld: filename=pyworld-0.2.4-cp36-cp36m-linux_x86_64.whl size=610421 sha256=40567466a238e127f440dbe4008823cd8aff8230b597fc348b6ecc14e9e05452\n","  Stored in directory: /root/.cache/pip/wheels/06/f9/b0/8a8b49ebc596329b34ee06798f192c27db09f42aff16a1cc82\n","Successfully built pyworld\n","Installing collected packages: pyworld\n","Successfully installed pyworld-0.2.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h-rByovrPXrk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605676495029,"user_tz":-180,"elapsed":40046,"user":{"displayName":"Aigul Sibgatullina","photoUrl":"","userId":"02965597584056384610"}},"outputId":"937dd34f-e1ea-4867-8f7f-36085c90bd9c"},"source":["!pip install tensorflow==1.12.0"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.12.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cc/ca70b78087015d21c5f3f93694107f34ebccb3be9624385a911d4b52ecef/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (83.1MB)\n","\u001b[K     |████████████████████████████████| 83.1MB 79kB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.15.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.3.3)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.1.2)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.8.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.1.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.33.2)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.18.5)\n","Collecting tensorboard<1.13.0,>=1.12.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.1MB 44.2MB/s \n","\u001b[?25hCollecting keras-applications>=1.0.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.0MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (3.12.4)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.35.1)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.10.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (3.3.3)\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (1.0.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12.0) (2.10.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12.0) (50.3.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (2.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (3.4.0)\n","Installing collected packages: tensorboard, keras-applications, tensorflow\n","  Found existing installation: tensorboard 2.3.0\n","    Uninstalling tensorboard-2.3.0:\n","      Successfully uninstalled tensorboard-2.3.0\n","  Found existing installation: tensorflow 2.3.0\n","    Uninstalling tensorflow-2.3.0:\n","      Successfully uninstalled tensorflow-2.3.0\n","Successfully installed keras-applications-1.0.8 tensorboard-1.12.2 tensorflow-1.12.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TQdnuug7Bxzg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605676898349,"user_tz":-180,"elapsed":686,"user":{"displayName":"Aigul Sibgatullina","photoUrl":"","userId":"02965597584056384610"}},"outputId":"0b6f152d-857e-4e61-aaf9-220d38685147"},"source":["# If this is your first time, run the git clone command after uncommenting it.\n","\n","# % cd /content/drive/My Drive/\n","# !git clone https://github.com/sora-12/Tobigs_music_project # code\n","\n","\n","% cd /content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer\n","!ls # Check if the code has been downloaded properly"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer\n","config.py\tdata\t model\t\t__pycache__  test.py\n","cycle_began.py\texample  my_models\tresult\t     train.py\n","cyclegan.py\tlog\t preprocess.py\tsample\t     Utils\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A3iIgLcL3nAS"},"source":["> # Please read the precautions and proceed!\n","\n","---\n","\n","> # train Precautions\n","\n","- data/train In the folder, put data containing only the vocal of the singer you want in A and B respectively.\n","- data If there are .npz and pickle files in the folder, delete them.\n","- model Please also empty the folder.\n","- Please check if the runtime environment is GPU.\n","\n","---\n","\n","> # train Caution\n","\n","- preprocess It takes a long time... Please wait... When the .npz file and the pickle file are created in the data folder, it is completed.\n","- Saves model checkpoints and weights every 100 epochs. It is a capacity of 1 to 2 gigabytes, so if the Google Drive capacity is insufficient, please proceed by erasing. (Meta, index, data is a set of 3 files.)\n","- Even if it ends in the middle, you don't have to worry about it because the preprocessing complete file and checkpoint are loaded and processed.\n","- If you have modified the code, it's nice! Please initialize and run the runtime. This is because there is a high probability that the synchronization will not work correctly!\n","\n","---\n","\n","> # test Precautions\n","\n","- data/test Create a folder by putting both the files of singer a and singer b. I'm still confused about A->B, B->A...\n","- sample The finished file will be created in the folder.\n","\n","---\n","\n","#### Each person will share by recording how they ran their hyperparameters, what performed good or badly!"]},{"cell_type":"markdown","metadata":{"id":"aImgBNx1CdOJ"},"source":["# Train"]},{"cell_type":"code","metadata":{"id":"oTLHObrOCaJR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605663765162,"user_tz":-180,"elapsed":26827578,"user":{"displayName":"Aigul Sibgatullina","photoUrl":"","userId":"02965597584056384610"}},"outputId":"ecbd2bea-e543-4c6e-c112-bb85d42bf062"},"source":["# data/train/A, B Save each vocal file with mr removed in\n","!python train.py"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","Constructing MCEPs....\n","files\n","['/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/data/train/A/Evanescence - Call Me When Youre Sober_vocals.wav', '/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/data/train/A/Evanescence - Everybodys Fool.wav_vocals.wav', '/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/data/train/A/Evanescence - Going Under.wav_vocals.wav', '/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/data/train/A/Evanescence - Hello.wav_vocals.wav', '/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/data/train/A/Evanescence - My Heart Is Broken.wav_vocals.wav', '/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/data/train/A/Evanescence - My Immortal.wav_vocals.wav', '/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/data/train/A/Evanescence - Taking Over Me.wav_vocals.wav']\n","Wave Loading Complete\n","files\n","['/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/data/train/B/Three Days Grace - Get Out Alive.wav_vocals.wav', '/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/data/train/B/Three Days Grace - Love Me Or Leave Me.wav_vocals.wav', '/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/data/train/B/Three Days Grace - Never Too Late.wav_vocals.wav', '/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/data/train/B/Three Days Grace - Nothing To Lose But You.wav_vocals.wav', '/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/data/train/B/Three Days Grace - Over and Over.wav_vocals.wav', '/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/data/train/B/Three Days Grace - Pain.wav_vocals.wav']\n","Wave Loading Complete\n","my_point\n","<generator object load_wavs.<locals>.<genexpr> at 0x7f025dd719e8> <generator object load_wavs.<locals>.<genexpr> at 0x7f025dd71fc0>\n","Decompose Generator Create!\n","Encoding....... Это занимает много времени .... Пожалуйста, подождите..... \n","0Кодирование 1-го файла завершено\n","1Кодирование 1-го файла завершено\n","2Кодирование 1-го файла завершено\n","3Кодирование 1-го файла завершено\n","4Кодирование 1-го файла завершено\n","5Кодирование 1-го файла завершено\n","6Кодирование 1-го файла завершено\n","==== Кодирование всех файлов ====\n","Decompose Generator Create!\n","Encoding....... Это занимает много времени .... Пожалуйста, подождите..... \n","0Кодирование 1-го файла завершено\n","1Кодирование 1-го файла завершено\n","2Кодирование 1-го файла завершено\n","3Кодирование 1-го файла завершено\n","4Кодирование 1-го файла завершено\n","5Кодирование 1-го файла завершено\n","==== Кодирование всех файлов ====\n","Constructing norm.pickle....\n","Constructing mcep.npz....\n","Constructing Log_f0s....\n","Preprocessing Done!!!\n","2020-11-17 18:18:33.943003: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","============================== train ==============================\n","ckpt!!! None\n","No CheckPoint, Train을 처음부터 하겠습니다!\n","Start Training...\n","Epoch : 0 \n","Generator Loss : 27.075962, Discriminator Loss : 0.868414, Time : 01:21\n","0 model save complete\n","Epoch : 1 \n","Generator Loss : 25.669678, Discriminator Loss : 0.667154, Time : 00:55\n","Epoch : 2 \n","Generator Loss : 21.698673, Discriminator Loss : 0.711666, Time : 00:52\n","Epoch : 3 \n","Generator Loss : 20.334164, Discriminator Loss : 0.642625, Time : 00:52\n","Epoch : 4 \n","Generator Loss : 19.939575, Discriminator Loss : 0.583633, Time : 00:52\n","Epoch : 5 \n","Generator Loss : 20.981030, Discriminator Loss : 0.454061, Time : 00:52\n","Epoch : 6 \n","Generator Loss : 23.207869, Discriminator Loss : 0.242542, Time : 00:52\n","Epoch : 7 \n","Generator Loss : 22.092232, Discriminator Loss : 0.222689, Time : 00:52\n","Epoch : 8 \n","Generator Loss : 20.386370, Discriminator Loss : 0.169611, Time : 00:52\n","Epoch : 9 \n","Generator Loss : 22.205614, Discriminator Loss : 0.114975, Time : 00:52\n","Epoch : 10 \n","Generator Loss : 22.017330, Discriminator Loss : 0.115110, Time : 00:52\n","Epoch : 11 \n","Generator Loss : 23.435589, Discriminator Loss : 0.039050, Time : 00:52\n","Epoch : 12 \n","Generator Loss : 23.675327, Discriminator Loss : 0.058834, Time : 00:52\n","Epoch : 13 \n","Generator Loss : 24.590298, Discriminator Loss : 0.039092, Time : 00:52\n","Epoch : 14 \n","Generator Loss : 21.146370, Discriminator Loss : 0.065463, Time : 00:52\n","Epoch : 15 \n","Generator Loss : 18.865761, Discriminator Loss : 0.098216, Time : 00:52\n","Epoch : 16 \n","Generator Loss : 20.388950, Discriminator Loss : 0.035810, Time : 00:52\n","Epoch : 17 \n","Generator Loss : 23.858276, Discriminator Loss : 0.041205, Time : 00:52\n","Epoch : 18 \n","Generator Loss : 21.142368, Discriminator Loss : 0.045420, Time : 00:52\n","Epoch : 19 \n","Generator Loss : 21.268019, Discriminator Loss : 0.013203, Time : 00:52\n","Epoch : 20 \n","Generator Loss : 22.491489, Discriminator Loss : 0.114093, Time : 00:52\n","Epoch : 21 \n","Generator Loss : 28.141235, Discriminator Loss : 0.018872, Time : 00:52\n","Epoch : 22 \n","Generator Loss : 18.894819, Discriminator Loss : 0.033141, Time : 00:52\n","Epoch : 23 \n","Generator Loss : 21.754198, Discriminator Loss : 0.027057, Time : 00:52\n","Epoch : 24 \n","Generator Loss : 21.145496, Discriminator Loss : 0.019759, Time : 00:52\n","Epoch : 25 \n","Generator Loss : 22.512367, Discriminator Loss : 0.015805, Time : 00:52\n","Epoch : 26 \n","Generator Loss : 19.676310, Discriminator Loss : 0.045069, Time : 00:52\n","Epoch : 27 \n","Generator Loss : 21.629271, Discriminator Loss : 0.053540, Time : 00:52\n","Epoch : 28 \n","Generator Loss : 17.766766, Discriminator Loss : 0.153328, Time : 00:52\n","Epoch : 29 \n","Generator Loss : 20.878799, Discriminator Loss : 0.021508, Time : 00:52\n","Epoch : 30 \n","Generator Loss : 23.043417, Discriminator Loss : 0.012752, Time : 00:52\n","Epoch : 31 \n","Generator Loss : 24.670401, Discriminator Loss : 0.039100, Time : 00:52\n","Epoch : 32 \n","Generator Loss : 21.755661, Discriminator Loss : 0.096848, Time : 00:52\n","Epoch : 33 \n","Generator Loss : 17.911352, Discriminator Loss : 0.037290, Time : 00:52\n","Epoch : 34 \n","Generator Loss : 22.255550, Discriminator Loss : 0.014146, Time : 00:52\n","Epoch : 35 \n","Generator Loss : 26.892803, Discriminator Loss : 0.005743, Time : 00:52\n","Epoch : 36 \n","Generator Loss : 21.562149, Discriminator Loss : 0.004887, Time : 00:53\n","Epoch : 37 \n","Generator Loss : 21.576815, Discriminator Loss : 0.034261, Time : 00:53\n","Epoch : 38 \n","Generator Loss : 16.417940, Discriminator Loss : 0.008503, Time : 00:53\n","Epoch : 39 \n","Generator Loss : 21.989840, Discriminator Loss : 0.006383, Time : 00:53\n","Epoch : 40 \n","Generator Loss : 21.062065, Discriminator Loss : 0.012477, Time : 00:53\n","Epoch : 41 \n","Generator Loss : 24.069809, Discriminator Loss : 0.009487, Time : 00:52\n","Epoch : 42 \n","Generator Loss : 20.201546, Discriminator Loss : 0.051850, Time : 00:52\n","Epoch : 43 \n","Generator Loss : 19.843803, Discriminator Loss : 0.048234, Time : 00:52\n","Epoch : 44 \n","Generator Loss : 20.944897, Discriminator Loss : 0.020277, Time : 00:52\n","Epoch : 45 \n","Generator Loss : 19.329456, Discriminator Loss : 0.007953, Time : 00:52\n","Epoch : 46 \n","Generator Loss : 20.360455, Discriminator Loss : 0.025377, Time : 00:52\n","Epoch : 47 \n","Generator Loss : 22.233423, Discriminator Loss : 0.023803, Time : 00:52\n","Epoch : 48 \n","Generator Loss : 21.269627, Discriminator Loss : 0.014303, Time : 00:52\n","Epoch : 49 \n","Generator Loss : 16.976353, Discriminator Loss : 0.006465, Time : 00:52\n","Epoch : 50 \n","Generator Loss : 19.796520, Discriminator Loss : 0.039124, Time : 00:52\n","Epoch : 51 \n","Generator Loss : 19.120230, Discriminator Loss : 0.003921, Time : 00:54\n","Epoch : 52 \n","Generator Loss : 20.695280, Discriminator Loss : 0.027810, Time : 00:52\n","Epoch : 53 \n","Generator Loss : 21.625950, Discriminator Loss : 0.023303, Time : 00:52\n","Epoch : 54 \n","Generator Loss : 21.015354, Discriminator Loss : 0.136242, Time : 00:52\n","Epoch : 55 \n","Generator Loss : 19.245558, Discriminator Loss : 0.018892, Time : 00:52\n","Epoch : 56 \n","Generator Loss : 17.769789, Discriminator Loss : 0.010114, Time : 00:52\n","Epoch : 57 \n","Generator Loss : 20.803892, Discriminator Loss : 0.039941, Time : 00:53\n","Epoch : 58 \n","Generator Loss : 17.182076, Discriminator Loss : 0.038424, Time : 00:53\n","Epoch : 59 \n","Generator Loss : 22.403835, Discriminator Loss : 0.014654, Time : 00:52\n","Epoch : 60 \n","Generator Loss : 18.046150, Discriminator Loss : 0.025258, Time : 00:52\n","Epoch : 61 \n","Generator Loss : 21.414040, Discriminator Loss : 0.011020, Time : 00:53\n","Epoch : 62 \n","Generator Loss : 19.525505, Discriminator Loss : 0.021982, Time : 00:52\n","Epoch : 63 \n","Generator Loss : 21.724949, Discriminator Loss : 0.018293, Time : 00:52\n","Epoch : 64 \n","Generator Loss : 16.778364, Discriminator Loss : 0.040878, Time : 00:52\n","Epoch : 65 \n","Generator Loss : 17.321831, Discriminator Loss : 0.334165, Time : 00:52\n","Epoch : 66 \n","Generator Loss : 20.248709, Discriminator Loss : 0.098717, Time : 00:52\n","Epoch : 67 \n","Generator Loss : 17.506447, Discriminator Loss : 0.009513, Time : 00:53\n","Epoch : 68 \n","Generator Loss : 19.368721, Discriminator Loss : 0.013768, Time : 00:53\n","Epoch : 69 \n","Generator Loss : 17.214264, Discriminator Loss : 0.032013, Time : 00:53\n","Epoch : 70 \n","Generator Loss : 20.698025, Discriminator Loss : 0.003014, Time : 00:53\n","Epoch : 71 \n","Generator Loss : 17.952364, Discriminator Loss : 0.009484, Time : 00:53\n","Epoch : 72 \n","Generator Loss : 19.667973, Discriminator Loss : 0.028224, Time : 00:52\n","Epoch : 73 \n","Generator Loss : 17.565269, Discriminator Loss : 0.020038, Time : 00:52\n","Epoch : 74 \n","Generator Loss : 20.642155, Discriminator Loss : 0.023510, Time : 00:52\n","Epoch : 75 \n","Generator Loss : 18.435268, Discriminator Loss : 0.021857, Time : 00:52\n","Epoch : 76 \n","Generator Loss : 24.299238, Discriminator Loss : 0.015119, Time : 00:52\n","Epoch : 77 \n","Generator Loss : 19.605812, Discriminator Loss : 0.010700, Time : 00:52\n","Epoch : 78 \n","Generator Loss : 16.042187, Discriminator Loss : 0.008343, Time : 00:52\n","Epoch : 79 \n","Generator Loss : 18.909956, Discriminator Loss : 0.041055, Time : 00:52\n","Epoch : 80 \n","Generator Loss : 19.774078, Discriminator Loss : 0.005803, Time : 00:52\n","Epoch : 81 \n","Generator Loss : 20.043213, Discriminator Loss : 0.009980, Time : 00:52\n","Epoch : 82 \n","Generator Loss : 16.440559, Discriminator Loss : 0.175800, Time : 00:52\n","Epoch : 83 \n","Generator Loss : 19.963528, Discriminator Loss : 0.039672, Time : 00:52\n","Epoch : 84 \n","Generator Loss : 17.004452, Discriminator Loss : 0.036617, Time : 00:52\n","Epoch : 85 \n","Generator Loss : 21.951969, Discriminator Loss : 0.040197, Time : 00:52\n","Epoch : 86 \n","Generator Loss : 19.173656, Discriminator Loss : 0.019730, Time : 00:52\n","Epoch : 87 \n","Generator Loss : 20.254494, Discriminator Loss : 0.176481, Time : 00:52\n","Epoch : 88 \n","Generator Loss : 19.057600, Discriminator Loss : 0.038924, Time : 00:52\n","Epoch : 89 \n","Generator Loss : 18.881195, Discriminator Loss : 0.064404, Time : 00:52\n","Epoch : 90 \n","Generator Loss : 17.236073, Discriminator Loss : 0.310184, Time : 00:52\n","Epoch : 91 \n","Generator Loss : 21.754257, Discriminator Loss : 0.051063, Time : 00:52\n","Epoch : 92 \n","Generator Loss : 19.200533, Discriminator Loss : 0.198605, Time : 00:52\n","Epoch : 93 \n","Generator Loss : 18.655752, Discriminator Loss : 0.226293, Time : 00:52\n","Epoch : 94 \n","Generator Loss : 19.279507, Discriminator Loss : 0.030631, Time : 00:52\n","Epoch : 95 \n","Generator Loss : 18.682283, Discriminator Loss : 0.028019, Time : 00:52\n","Epoch : 96 \n","Generator Loss : 19.704376, Discriminator Loss : 0.013685, Time : 00:52\n","Epoch : 97 \n","Generator Loss : 19.985325, Discriminator Loss : 0.015123, Time : 00:52\n","Epoch : 98 \n","Generator Loss : 21.711056, Discriminator Loss : 0.028154, Time : 00:52\n","Epoch : 99 \n","Generator Loss : 18.287598, Discriminator Loss : 0.090471, Time : 00:53\n","Epoch : 100 \n","Generator Loss : 19.123108, Discriminator Loss : 0.014398, Time : 00:53\n","100 model save complete\n","Epoch : 101 \n","Generator Loss : 17.593391, Discriminator Loss : 0.107720, Time : 00:56\n","Epoch : 102 \n","Generator Loss : 18.824110, Discriminator Loss : 0.013221, Time : 00:53\n","Epoch : 103 \n","Generator Loss : 21.368269, Discriminator Loss : 0.137628, Time : 00:53\n","Epoch : 104 \n","Generator Loss : 20.170956, Discriminator Loss : 0.048775, Time : 00:52\n","Epoch : 105 \n","Generator Loss : 17.385260, Discriminator Loss : 0.135279, Time : 00:52\n","Epoch : 106 \n","Generator Loss : 20.423544, Discriminator Loss : 0.060634, Time : 00:52\n","Epoch : 107 \n","Generator Loss : 18.867649, Discriminator Loss : 0.064325, Time : 00:52\n","Epoch : 108 \n","Generator Loss : 18.903282, Discriminator Loss : 0.063460, Time : 00:52\n","Epoch : 109 \n","Generator Loss : 17.210110, Discriminator Loss : 0.089268, Time : 00:52\n","Epoch : 110 \n","Generator Loss : 20.736374, Discriminator Loss : 0.059757, Time : 00:52\n","Epoch : 111 \n","Generator Loss : 23.346134, Discriminator Loss : 0.073026, Time : 00:52\n","Epoch : 112 \n","Generator Loss : 17.747276, Discriminator Loss : 0.124811, Time : 00:52\n","Epoch : 113 \n","Generator Loss : 19.661625, Discriminator Loss : 0.171599, Time : 00:52\n","Epoch : 114 \n","Generator Loss : 19.329941, Discriminator Loss : 0.067654, Time : 00:52\n","Epoch : 115 \n","Generator Loss : 16.959993, Discriminator Loss : 0.110612, Time : 00:52\n","Epoch : 116 \n","Generator Loss : 18.034718, Discriminator Loss : 0.104020, Time : 00:52\n","Epoch : 117 \n","Generator Loss : 19.938213, Discriminator Loss : 0.301816, Time : 00:52\n","Epoch : 118 \n","Generator Loss : 20.323318, Discriminator Loss : 0.094869, Time : 00:52\n","Epoch : 119 \n","Generator Loss : 19.576515, Discriminator Loss : 0.062718, Time : 00:52\n","Epoch : 120 \n","Generator Loss : 18.252552, Discriminator Loss : 0.111022, Time : 00:52\n","Epoch : 121 \n","Generator Loss : 17.928993, Discriminator Loss : 0.069556, Time : 00:53\n","Epoch : 122 \n","Generator Loss : 16.826382, Discriminator Loss : 0.111269, Time : 00:52\n","Epoch : 123 \n","Generator Loss : 18.465816, Discriminator Loss : 0.039543, Time : 00:52\n","Epoch : 124 \n","Generator Loss : 21.175085, Discriminator Loss : 0.072409, Time : 00:53\n","Epoch : 125 \n","Generator Loss : 17.897728, Discriminator Loss : 0.045190, Time : 00:52\n","Epoch : 126 \n","Generator Loss : 17.725237, Discriminator Loss : 0.072468, Time : 00:52\n","Epoch : 127 \n","Generator Loss : 19.789856, Discriminator Loss : 0.176561, Time : 00:52\n","Epoch : 128 \n","Generator Loss : 17.618032, Discriminator Loss : 0.064635, Time : 00:52\n","Epoch : 129 \n","Generator Loss : 23.902933, Discriminator Loss : 0.252237, Time : 00:52\n","Epoch : 130 \n","Generator Loss : 19.613726, Discriminator Loss : 0.047219, Time : 00:53\n","Epoch : 131 \n","Generator Loss : 20.836666, Discriminator Loss : 0.855410, Time : 00:53\n","Epoch : 132 \n","Generator Loss : 18.228874, Discriminator Loss : 0.189034, Time : 00:53\n","Epoch : 133 \n","Generator Loss : 19.750530, Discriminator Loss : 0.143536, Time : 00:53\n","Epoch : 134 \n","Generator Loss : 17.701454, Discriminator Loss : 0.045645, Time : 00:53\n","Epoch : 135 \n","Generator Loss : 20.804001, Discriminator Loss : 0.124778, Time : 00:52\n","Epoch : 136 \n","Generator Loss : 17.468901, Discriminator Loss : 0.073119, Time : 00:52\n","Epoch : 137 \n","Generator Loss : 16.768097, Discriminator Loss : 0.063366, Time : 00:52\n","Epoch : 138 \n","Generator Loss : 17.212353, Discriminator Loss : 0.087693, Time : 00:52\n","Epoch : 139 \n","Generator Loss : 17.618023, Discriminator Loss : 0.367941, Time : 00:52\n","Epoch : 140 \n","Generator Loss : 18.014851, Discriminator Loss : 0.367425, Time : 00:52\n","Epoch : 141 \n","Generator Loss : 17.116266, Discriminator Loss : 0.191514, Time : 00:52\n","Epoch : 142 \n","Generator Loss : 17.450497, Discriminator Loss : 0.244033, Time : 00:52\n","Epoch : 143 \n","Generator Loss : 19.689257, Discriminator Loss : 0.151766, Time : 00:53\n","Epoch : 144 \n","Generator Loss : 18.283676, Discriminator Loss : 0.228248, Time : 00:52\n","Epoch : 145 \n","Generator Loss : 19.779503, Discriminator Loss : 0.342014, Time : 00:52\n","Epoch : 146 \n","Generator Loss : 15.214645, Discriminator Loss : 0.263438, Time : 00:52\n","Epoch : 147 \n","Generator Loss : 20.717558, Discriminator Loss : 0.295023, Time : 00:52\n","Epoch : 148 \n","Generator Loss : 17.752115, Discriminator Loss : 0.322003, Time : 00:52\n","Epoch : 149 \n","Generator Loss : 18.234890, Discriminator Loss : 0.408662, Time : 00:52\n","Epoch : 150 \n","Generator Loss : 18.326143, Discriminator Loss : 0.266784, Time : 00:52\n","Epoch : 151 \n","Generator Loss : 18.334702, Discriminator Loss : 0.287055, Time : 00:52\n","Epoch : 152 \n","Generator Loss : 18.911312, Discriminator Loss : 0.254482, Time : 00:52\n","Epoch : 153 \n","Generator Loss : 20.339474, Discriminator Loss : 0.208902, Time : 00:52\n","Epoch : 154 \n","Generator Loss : 20.157005, Discriminator Loss : 0.394418, Time : 00:52\n","Epoch : 155 \n","Generator Loss : 18.050507, Discriminator Loss : 0.334484, Time : 00:52\n","Epoch : 156 \n","Generator Loss : 17.246124, Discriminator Loss : 0.297195, Time : 00:52\n","Epoch : 157 \n","Generator Loss : 17.123661, Discriminator Loss : 0.258240, Time : 00:52\n","Epoch : 158 \n","Generator Loss : 17.646484, Discriminator Loss : 0.309309, Time : 00:52\n","Epoch : 159 \n","Generator Loss : 15.751503, Discriminator Loss : 0.212611, Time : 00:52\n","Epoch : 160 \n","Generator Loss : 19.075134, Discriminator Loss : 0.268083, Time : 00:53\n","Epoch : 161 \n","Generator Loss : 18.726254, Discriminator Loss : 0.305639, Time : 00:53\n","Epoch : 162 \n","Generator Loss : 24.865820, Discriminator Loss : 0.316430, Time : 00:53\n","Epoch : 163 \n","Generator Loss : 16.740499, Discriminator Loss : 0.329089, Time : 00:53\n","Epoch : 164 \n","Generator Loss : 18.133038, Discriminator Loss : 0.240855, Time : 00:53\n","Epoch : 165 \n","Generator Loss : 16.174913, Discriminator Loss : 0.154659, Time : 00:52\n","Epoch : 166 \n","Generator Loss : 16.786409, Discriminator Loss : 0.369551, Time : 00:52\n","Epoch : 167 \n","Generator Loss : 18.111813, Discriminator Loss : 0.218220, Time : 00:52\n","Epoch : 168 \n","Generator Loss : 15.635899, Discriminator Loss : 0.302459, Time : 00:52\n","Epoch : 169 \n","Generator Loss : 20.625797, Discriminator Loss : 0.164288, Time : 00:52\n","Epoch : 170 \n","Generator Loss : 21.597033, Discriminator Loss : 0.256771, Time : 00:52\n","Epoch : 171 \n","Generator Loss : 16.688358, Discriminator Loss : 0.292779, Time : 00:52\n","Epoch : 172 \n","Generator Loss : 20.031780, Discriminator Loss : 0.242188, Time : 00:52\n","Epoch : 173 \n","Generator Loss : 17.888426, Discriminator Loss : 0.355974, Time : 00:52\n","Epoch : 174 \n","Generator Loss : 16.770821, Discriminator Loss : 0.211633, Time : 00:53\n","Epoch : 175 \n","Generator Loss : 15.350096, Discriminator Loss : 0.189707, Time : 00:52\n","Epoch : 176 \n","Generator Loss : 16.490019, Discriminator Loss : 0.200226, Time : 00:52\n","Epoch : 177 \n","Generator Loss : 17.319538, Discriminator Loss : 0.123634, Time : 00:52\n","Epoch : 178 \n","Generator Loss : 18.776180, Discriminator Loss : 0.275127, Time : 00:52\n","Epoch : 179 \n","Generator Loss : 19.576824, Discriminator Loss : 0.542099, Time : 00:53\n","Epoch : 180 \n","Generator Loss : 14.933828, Discriminator Loss : 0.169967, Time : 00:52\n","Epoch : 181 \n","Generator Loss : 17.919794, Discriminator Loss : 0.172779, Time : 00:53\n","Epoch : 182 \n","Generator Loss : 18.172783, Discriminator Loss : 0.166153, Time : 00:53\n","Epoch : 183 \n","Generator Loss : 15.566847, Discriminator Loss : 0.262470, Time : 00:53\n","Epoch : 184 \n","Generator Loss : 23.424608, Discriminator Loss : 0.218709, Time : 00:53\n","Epoch : 185 \n","Generator Loss : 18.042858, Discriminator Loss : 0.246468, Time : 00:53\n","Epoch : 186 \n","Generator Loss : 17.699749, Discriminator Loss : 0.100668, Time : 00:52\n","Epoch : 187 \n","Generator Loss : 16.955116, Discriminator Loss : 0.346007, Time : 00:52\n","Epoch : 188 \n","Generator Loss : 23.335175, Discriminator Loss : 0.154113, Time : 00:52\n","Epoch : 189 \n","Generator Loss : 15.782977, Discriminator Loss : 0.224239, Time : 00:52\n","Epoch : 190 \n","Generator Loss : 15.670279, Discriminator Loss : 0.194455, Time : 00:52\n","Epoch : 191 \n","Generator Loss : 16.568554, Discriminator Loss : 0.175630, Time : 00:53\n","Epoch : 192 \n","Generator Loss : 16.779398, Discriminator Loss : 0.161503, Time : 00:53\n","Epoch : 193 \n","Generator Loss : 18.310942, Discriminator Loss : 0.079159, Time : 00:53\n","Epoch : 194 \n","Generator Loss : 16.980461, Discriminator Loss : 0.321554, Time : 00:53\n","Epoch : 195 \n","Generator Loss : 16.484173, Discriminator Loss : 0.254363, Time : 00:53\n","Epoch : 196 \n","Generator Loss : 16.946388, Discriminator Loss : 0.186749, Time : 00:52\n","Epoch : 197 \n","Generator Loss : 16.957808, Discriminator Loss : 0.221862, Time : 00:53\n","Epoch : 198 \n","Generator Loss : 19.474773, Discriminator Loss : 0.211991, Time : 00:52\n","Epoch : 199 \n","Generator Loss : 17.832794, Discriminator Loss : 0.144166, Time : 00:52\n","Epoch : 200 \n","Generator Loss : 23.654453, Discriminator Loss : 0.364575, Time : 00:52\n","200 model save complete\n","Epoch : 201 \n","Generator Loss : 18.894575, Discriminator Loss : 0.264224, Time : 00:55\n","Epoch : 202 \n","Generator Loss : 17.743292, Discriminator Loss : 0.139201, Time : 00:53\n","Epoch : 203 \n","Generator Loss : 15.569705, Discriminator Loss : 0.148697, Time : 00:52\n","Epoch : 204 \n","Generator Loss : 20.290773, Discriminator Loss : 0.093006, Time : 00:53\n","Epoch : 205 \n","Generator Loss : 15.223531, Discriminator Loss : 0.214715, Time : 00:52\n","Epoch : 206 \n","Generator Loss : 17.625198, Discriminator Loss : 0.200793, Time : 00:52\n","Epoch : 207 \n","Generator Loss : 17.329956, Discriminator Loss : 0.200291, Time : 00:52\n","Epoch : 208 \n","Generator Loss : 23.715452, Discriminator Loss : 0.120041, Time : 00:52\n","Epoch : 209 \n","Generator Loss : 19.563047, Discriminator Loss : 0.142168, Time : 00:52\n","Epoch : 210 \n","Generator Loss : 16.846241, Discriminator Loss : 0.321846, Time : 00:52\n","Epoch : 211 \n","Generator Loss : 20.706205, Discriminator Loss : 0.341985, Time : 00:53\n","Epoch : 212 \n","Generator Loss : 23.123005, Discriminator Loss : 0.077686, Time : 00:53\n","Epoch : 213 \n","Generator Loss : 18.872192, Discriminator Loss : 0.112364, Time : 00:53\n","Epoch : 214 \n","Generator Loss : 16.567690, Discriminator Loss : 0.172324, Time : 00:52\n","Epoch : 215 \n","Generator Loss : 20.602020, Discriminator Loss : 0.195223, Time : 00:52\n","Epoch : 216 \n","Generator Loss : 19.661854, Discriminator Loss : 0.121699, Time : 00:52\n","Epoch : 217 \n","Generator Loss : 17.823450, Discriminator Loss : 0.259372, Time : 00:52\n","Epoch : 218 \n","Generator Loss : 17.297455, Discriminator Loss : 0.184140, Time : 00:52\n","Epoch : 219 \n","Generator Loss : 19.862600, Discriminator Loss : 0.162079, Time : 00:53\n","Epoch : 220 \n","Generator Loss : 18.445799, Discriminator Loss : 0.096011, Time : 00:53\n","Epoch : 221 \n","Generator Loss : 17.273148, Discriminator Loss : 0.083650, Time : 00:53\n","Epoch : 222 \n","Generator Loss : 21.702085, Discriminator Loss : 0.211287, Time : 00:52\n","Epoch : 223 \n","Generator Loss : 16.583195, Discriminator Loss : 0.161515, Time : 00:52\n","Epoch : 224 \n","Generator Loss : 16.997761, Discriminator Loss : 0.228749, Time : 00:52\n","Epoch : 225 \n","Generator Loss : 24.127560, Discriminator Loss : 0.205668, Time : 00:52\n","Epoch : 226 \n","Generator Loss : 16.385799, Discriminator Loss : 0.121004, Time : 00:52\n","Epoch : 227 \n","Generator Loss : 19.055983, Discriminator Loss : 0.096482, Time : 00:52\n","Epoch : 228 \n","Generator Loss : 16.848049, Discriminator Loss : 0.294243, Time : 00:52\n","Epoch : 229 \n","Generator Loss : 17.992037, Discriminator Loss : 0.155971, Time : 00:52\n","Epoch : 230 \n","Generator Loss : 21.551594, Discriminator Loss : 0.269158, Time : 00:52\n","Epoch : 231 \n","Generator Loss : 17.522078, Discriminator Loss : 0.267490, Time : 00:52\n","Epoch : 232 \n","Generator Loss : 16.672668, Discriminator Loss : 0.088672, Time : 00:52\n","Epoch : 233 \n","Generator Loss : 17.593218, Discriminator Loss : 0.120974, Time : 00:52\n","Epoch : 234 \n","Generator Loss : 19.528450, Discriminator Loss : 0.109297, Time : 00:52\n","Epoch : 235 \n","Generator Loss : 16.693214, Discriminator Loss : 0.298544, Time : 00:52\n","Epoch : 236 \n","Generator Loss : 18.685862, Discriminator Loss : 0.473827, Time : 00:53\n","Epoch : 237 \n","Generator Loss : 17.603809, Discriminator Loss : 0.307682, Time : 00:52\n","Epoch : 238 \n","Generator Loss : 17.196714, Discriminator Loss : 0.246356, Time : 00:53\n","Epoch : 239 \n","Generator Loss : 17.665630, Discriminator Loss : 0.213966, Time : 00:52\n","Epoch : 240 \n","Generator Loss : 20.958773, Discriminator Loss : 0.377098, Time : 00:53\n","Epoch : 241 \n","Generator Loss : 17.700546, Discriminator Loss : 0.408465, Time : 00:53\n","Epoch : 242 \n","Generator Loss : 23.498760, Discriminator Loss : 0.244674, Time : 00:52\n","Epoch : 243 \n","Generator Loss : 16.633081, Discriminator Loss : 0.069815, Time : 00:52\n","Epoch : 244 \n","Generator Loss : 18.046761, Discriminator Loss : 0.222247, Time : 00:52\n","Epoch : 245 \n","Generator Loss : 18.722719, Discriminator Loss : 0.207033, Time : 00:52\n","Epoch : 246 \n","Generator Loss : 16.190964, Discriminator Loss : 0.342866, Time : 00:52\n","Epoch : 247 \n","Generator Loss : 16.981943, Discriminator Loss : 0.282726, Time : 00:52\n","Epoch : 248 \n","Generator Loss : 21.815487, Discriminator Loss : 0.183570, Time : 00:52\n","Epoch : 249 \n","Generator Loss : 16.910143, Discriminator Loss : 0.139719, Time : 00:52\n","Epoch : 250 \n","Generator Loss : 17.383955, Discriminator Loss : 0.271478, Time : 00:53\n","Epoch : 251 \n","Generator Loss : 18.911037, Discriminator Loss : 0.193730, Time : 00:53\n","Epoch : 252 \n","Generator Loss : 18.143970, Discriminator Loss : 0.194190, Time : 00:52\n","Epoch : 253 \n","Generator Loss : 18.505966, Discriminator Loss : 0.128072, Time : 00:52\n","Epoch : 254 \n","Generator Loss : 15.605303, Discriminator Loss : 0.428795, Time : 00:52\n","Epoch : 255 \n","Generator Loss : 16.103699, Discriminator Loss : 0.256880, Time : 00:52\n","Epoch : 256 \n","Generator Loss : 14.836124, Discriminator Loss : 0.200323, Time : 00:53\n","Epoch : 257 \n","Generator Loss : 15.449087, Discriminator Loss : 0.376729, Time : 00:53\n","Epoch : 258 \n","Generator Loss : 14.603965, Discriminator Loss : 0.214626, Time : 00:53\n","Epoch : 259 \n","Generator Loss : 17.077810, Discriminator Loss : 0.390652, Time : 00:53\n","Epoch : 260 \n","Generator Loss : 17.016191, Discriminator Loss : 0.089395, Time : 00:53\n","Epoch : 261 \n","Generator Loss : 16.481007, Discriminator Loss : 0.257385, Time : 00:53\n","Epoch : 262 \n","Generator Loss : 14.923737, Discriminator Loss : 0.359660, Time : 00:52\n","Epoch : 263 \n","Generator Loss : 17.584637, Discriminator Loss : 0.126682, Time : 00:52\n","Epoch : 264 \n","Generator Loss : 17.594568, Discriminator Loss : 0.203064, Time : 00:53\n","Epoch : 265 \n","Generator Loss : 17.242176, Discriminator Loss : 0.252510, Time : 00:53\n","Epoch : 266 \n","Generator Loss : 18.477587, Discriminator Loss : 0.205067, Time : 00:53\n","Epoch : 267 \n","Generator Loss : 17.107018, Discriminator Loss : 0.412125, Time : 00:53\n","Epoch : 268 \n","Generator Loss : 19.285154, Discriminator Loss : 0.255005, Time : 00:53\n","Epoch : 269 \n","Generator Loss : 16.119095, Discriminator Loss : 0.277076, Time : 00:53\n","Epoch : 270 \n","Generator Loss : 16.872892, Discriminator Loss : 0.198680, Time : 00:53\n","Epoch : 271 \n","Generator Loss : 19.649197, Discriminator Loss : 0.248351, Time : 00:53\n","Epoch : 272 \n","Generator Loss : 17.589098, Discriminator Loss : 0.177103, Time : 00:53\n","Epoch : 273 \n","Generator Loss : 19.393019, Discriminator Loss : 0.294594, Time : 00:53\n","Epoch : 274 \n","Generator Loss : 20.480724, Discriminator Loss : 0.239634, Time : 00:52\n","Epoch : 275 \n","Generator Loss : 16.282835, Discriminator Loss : 0.285474, Time : 00:52\n","Epoch : 276 \n","Generator Loss : 18.033587, Discriminator Loss : 0.303525, Time : 00:52\n","Epoch : 277 \n","Generator Loss : 20.009792, Discriminator Loss : 0.502431, Time : 00:52\n","Epoch : 278 \n","Generator Loss : 15.937396, Discriminator Loss : 0.326384, Time : 00:53\n","Epoch : 279 \n","Generator Loss : 23.940430, Discriminator Loss : 0.635036, Time : 00:53\n","Epoch : 280 \n","Generator Loss : 16.473965, Discriminator Loss : 0.278595, Time : 00:52\n","Epoch : 281 \n","Generator Loss : 16.580708, Discriminator Loss : 0.331452, Time : 00:52\n","Epoch : 282 \n","Generator Loss : 17.816114, Discriminator Loss : 0.498431, Time : 00:52\n","Epoch : 283 \n","Generator Loss : 21.778217, Discriminator Loss : 0.404358, Time : 00:52\n","Epoch : 284 \n","Generator Loss : 17.109592, Discriminator Loss : 0.298205, Time : 00:53\n","Epoch : 285 \n","Generator Loss : 15.452003, Discriminator Loss : 0.248493, Time : 00:52\n","Epoch : 286 \n","Generator Loss : 22.118092, Discriminator Loss : 0.219077, Time : 00:53\n","Epoch : 287 \n","Generator Loss : 18.674759, Discriminator Loss : 0.313002, Time : 00:53\n","Epoch : 288 \n","Generator Loss : 20.835211, Discriminator Loss : 0.387531, Time : 00:53\n","Epoch : 289 \n","Generator Loss : 22.953863, Discriminator Loss : 0.217779, Time : 00:52\n","Epoch : 290 \n","Generator Loss : 16.432686, Discriminator Loss : 0.289803, Time : 00:52\n","Epoch : 291 \n","Generator Loss : 15.637971, Discriminator Loss : 0.237012, Time : 00:52\n","Epoch : 292 \n","Generator Loss : 18.683758, Discriminator Loss : 0.315503, Time : 00:52\n","Epoch : 293 \n","Generator Loss : 16.579720, Discriminator Loss : 0.383477, Time : 00:53\n","Epoch : 294 \n","Generator Loss : 17.203697, Discriminator Loss : 0.328186, Time : 00:53\n","Epoch : 295 \n","Generator Loss : 15.389547, Discriminator Loss : 0.377689, Time : 00:53\n","Epoch : 296 \n","Generator Loss : 16.088886, Discriminator Loss : 0.477636, Time : 00:53\n","Epoch : 297 \n","Generator Loss : 16.768763, Discriminator Loss : 0.236710, Time : 00:53\n","Epoch : 298 \n","Generator Loss : 19.141533, Discriminator Loss : 0.398954, Time : 00:53\n","Epoch : 299 \n","Generator Loss : 15.257483, Discriminator Loss : 0.246968, Time : 00:53\n","Epoch : 300 \n","Generator Loss : 16.410841, Discriminator Loss : 0.237312, Time : 00:53\n","300 model save complete\n","Epoch : 301 \n","Generator Loss : 17.820789, Discriminator Loss : 0.405305, Time : 00:56\n","Epoch : 302 \n","Generator Loss : 15.585666, Discriminator Loss : 0.401118, Time : 00:52\n","Epoch : 303 \n","Generator Loss : 14.889664, Discriminator Loss : 0.371180, Time : 00:53\n","Epoch : 304 \n","Generator Loss : 15.533010, Discriminator Loss : 0.420693, Time : 00:53\n","Epoch : 305 \n","Generator Loss : 20.643782, Discriminator Loss : 0.222610, Time : 00:53\n","Epoch : 306 \n","Generator Loss : 19.486259, Discriminator Loss : 0.352998, Time : 00:53\n","Epoch : 307 \n","Generator Loss : 16.692514, Discriminator Loss : 0.242106, Time : 00:53\n","Epoch : 308 \n","Generator Loss : 17.695675, Discriminator Loss : 0.203115, Time : 00:53\n","Epoch : 309 \n","Generator Loss : 18.882303, Discriminator Loss : 0.139652, Time : 00:53\n","Epoch : 310 \n","Generator Loss : 16.791004, Discriminator Loss : 0.174530, Time : 00:53\n","Epoch : 311 \n","Generator Loss : 16.898266, Discriminator Loss : 0.219979, Time : 00:55\n","Epoch : 312 \n","Generator Loss : 21.045460, Discriminator Loss : 0.208412, Time : 00:53\n","Epoch : 313 \n","Generator Loss : 15.610676, Discriminator Loss : 0.177278, Time : 00:53\n","Epoch : 314 \n","Generator Loss : 14.465104, Discriminator Loss : 0.168612, Time : 00:53\n","Epoch : 315 \n","Generator Loss : 15.186007, Discriminator Loss : 0.140521, Time : 00:53\n","Epoch : 316 \n","Generator Loss : 16.538631, Discriminator Loss : 0.291907, Time : 00:53\n","Epoch : 317 \n","Generator Loss : 16.989304, Discriminator Loss : 0.264164, Time : 00:53\n","Epoch : 318 \n","Generator Loss : 16.749588, Discriminator Loss : 0.325662, Time : 00:53\n","Epoch : 319 \n","Generator Loss : 17.718447, Discriminator Loss : 0.458300, Time : 00:53\n","Epoch : 320 \n","Generator Loss : 18.170263, Discriminator Loss : 0.214417, Time : 00:53\n","Epoch : 321 \n","Generator Loss : 15.436230, Discriminator Loss : 0.256952, Time : 00:53\n","Epoch : 322 \n","Generator Loss : 16.583092, Discriminator Loss : 0.234049, Time : 00:53\n","Epoch : 323 \n","Generator Loss : 16.461100, Discriminator Loss : 0.182148, Time : 00:53\n","Epoch : 324 \n","Generator Loss : 15.810605, Discriminator Loss : 0.440608, Time : 00:53\n","Epoch : 325 \n","Generator Loss : 16.057251, Discriminator Loss : 0.348025, Time : 00:53\n","Epoch : 326 \n","Generator Loss : 16.176762, Discriminator Loss : 0.489971, Time : 00:53\n","Epoch : 327 \n","Generator Loss : 19.203501, Discriminator Loss : 0.260379, Time : 00:54\n","Epoch : 328 \n","Generator Loss : 20.754082, Discriminator Loss : 0.208142, Time : 00:54\n","Epoch : 329 \n","Generator Loss : 17.616255, Discriminator Loss : 0.152331, Time : 00:54\n","Epoch : 330 \n","Generator Loss : 16.249443, Discriminator Loss : 0.203389, Time : 00:53\n","Epoch : 331 \n","Generator Loss : 18.454758, Discriminator Loss : 0.406929, Time : 00:53\n","Epoch : 332 \n","Generator Loss : 16.842714, Discriminator Loss : 0.124852, Time : 00:53\n","Epoch : 333 \n","Generator Loss : 16.716497, Discriminator Loss : 0.172481, Time : 00:53\n","Epoch : 334 \n","Generator Loss : 14.509751, Discriminator Loss : 0.278839, Time : 00:53\n","Epoch : 335 \n","Generator Loss : 18.113022, Discriminator Loss : 0.196561, Time : 00:53\n","Epoch : 336 \n","Generator Loss : 15.696823, Discriminator Loss : 0.222401, Time : 00:53\n","Epoch : 337 \n","Generator Loss : 16.877092, Discriminator Loss : 0.394711, Time : 00:53\n","Epoch : 338 \n","Generator Loss : 17.637936, Discriminator Loss : 0.264189, Time : 00:53\n","Epoch : 339 \n","Generator Loss : 15.425042, Discriminator Loss : 0.284639, Time : 00:53\n","Epoch : 340 \n","Generator Loss : 17.554798, Discriminator Loss : 0.202774, Time : 00:53\n","Epoch : 341 \n","Generator Loss : 17.962925, Discriminator Loss : 0.273401, Time : 00:53\n","Epoch : 342 \n","Generator Loss : 19.937090, Discriminator Loss : 0.449569, Time : 00:52\n","Epoch : 343 \n","Generator Loss : 16.582165, Discriminator Loss : 0.261746, Time : 00:53\n","Epoch : 344 \n","Generator Loss : 21.004646, Discriminator Loss : 0.153005, Time : 00:53\n","Epoch : 345 \n","Generator Loss : 17.532196, Discriminator Loss : 0.248883, Time : 00:52\n","Epoch : 346 \n","Generator Loss : 12.696986, Discriminator Loss : 0.235129, Time : 00:52\n","Epoch : 347 \n","Generator Loss : 16.191917, Discriminator Loss : 0.201187, Time : 00:52\n","Epoch : 348 \n","Generator Loss : 18.471436, Discriminator Loss : 0.344178, Time : 00:53\n","Epoch : 349 \n","Generator Loss : 16.392330, Discriminator Loss : 0.208942, Time : 00:53\n","Epoch : 350 \n","Generator Loss : 17.082840, Discriminator Loss : 0.240255, Time : 00:53\n","Epoch : 351 \n","Generator Loss : 23.677193, Discriminator Loss : 0.329642, Time : 00:53\n","Epoch : 352 \n","Generator Loss : 16.329569, Discriminator Loss : 0.378373, Time : 00:52\n","Epoch : 353 \n","Generator Loss : 14.584645, Discriminator Loss : 0.327839, Time : 00:52\n","Epoch : 354 \n","Generator Loss : 16.314787, Discriminator Loss : 0.221630, Time : 00:53\n","Epoch : 355 \n","Generator Loss : 19.075123, Discriminator Loss : 0.293133, Time : 00:52\n","Epoch : 356 \n","Generator Loss : 17.232277, Discriminator Loss : 0.307818, Time : 00:53\n","Epoch : 357 \n","Generator Loss : 21.941963, Discriminator Loss : 0.273362, Time : 00:53\n","Epoch : 358 \n","Generator Loss : 14.571817, Discriminator Loss : 0.141606, Time : 00:53\n","Epoch : 359 \n","Generator Loss : 17.707603, Discriminator Loss : 0.228747, Time : 00:53\n","Epoch : 360 \n","Generator Loss : 17.871672, Discriminator Loss : 0.484218, Time : 00:53\n","Epoch : 361 \n","Generator Loss : 17.243448, Discriminator Loss : 0.237128, Time : 00:53\n","Epoch : 362 \n","Generator Loss : 19.433395, Discriminator Loss : 0.196324, Time : 00:53\n","Epoch : 363 \n","Generator Loss : 18.650684, Discriminator Loss : 0.457862, Time : 00:53\n","Epoch : 364 \n","Generator Loss : 17.325211, Discriminator Loss : 0.502391, Time : 00:53\n","Epoch : 365 \n","Generator Loss : 22.181078, Discriminator Loss : 0.247396, Time : 00:53\n","Epoch : 366 \n","Generator Loss : 16.581903, Discriminator Loss : 0.520874, Time : 00:53\n","Epoch : 367 \n","Generator Loss : 16.205553, Discriminator Loss : 0.249349, Time : 00:53\n","Epoch : 368 \n","Generator Loss : 16.929146, Discriminator Loss : 0.326421, Time : 00:52\n","Epoch : 369 \n","Generator Loss : 16.229462, Discriminator Loss : 0.284517, Time : 00:53\n","Epoch : 370 \n","Generator Loss : 17.242054, Discriminator Loss : 0.242685, Time : 00:53\n","Epoch : 371 \n","Generator Loss : 15.519394, Discriminator Loss : 0.291774, Time : 00:53\n","Epoch : 372 \n","Generator Loss : 15.497046, Discriminator Loss : 0.324416, Time : 00:53\n","Epoch : 373 \n","Generator Loss : 14.627300, Discriminator Loss : 0.201018, Time : 00:53\n","Epoch : 374 \n","Generator Loss : 15.866727, Discriminator Loss : 0.266852, Time : 00:53\n","Epoch : 375 \n","Generator Loss : 19.790302, Discriminator Loss : 0.529310, Time : 00:53\n","Epoch : 376 \n","Generator Loss : 15.924534, Discriminator Loss : 0.184161, Time : 00:53\n","Epoch : 377 \n","Generator Loss : 19.032791, Discriminator Loss : 0.313881, Time : 00:53\n","Epoch : 378 \n","Generator Loss : 20.357229, Discriminator Loss : 0.237266, Time : 00:53\n","Epoch : 379 \n","Generator Loss : 16.329765, Discriminator Loss : 0.319606, Time : 00:53\n","Epoch : 380 \n","Generator Loss : 18.086487, Discriminator Loss : 0.319111, Time : 00:53\n","Epoch : 381 \n","Generator Loss : 18.196259, Discriminator Loss : 0.179800, Time : 00:53\n","Epoch : 382 \n","Generator Loss : 13.999578, Discriminator Loss : 0.399306, Time : 00:53\n","Epoch : 383 \n","Generator Loss : 18.093008, Discriminator Loss : 0.208287, Time : 00:53\n","Epoch : 384 \n","Generator Loss : 18.852713, Discriminator Loss : 0.250168, Time : 00:54\n","Epoch : 385 \n","Generator Loss : 16.702778, Discriminator Loss : 0.275691, Time : 00:53\n","Epoch : 386 \n","Generator Loss : 14.899126, Discriminator Loss : 0.415776, Time : 00:53\n","Epoch : 387 \n","Generator Loss : 14.123873, Discriminator Loss : 0.394703, Time : 00:53\n","Epoch : 388 \n","Generator Loss : 15.960716, Discriminator Loss : 0.316088, Time : 00:53\n","Epoch : 389 \n","Generator Loss : 13.878472, Discriminator Loss : 0.271767, Time : 00:53\n","Epoch : 390 \n","Generator Loss : 15.575911, Discriminator Loss : 0.442668, Time : 00:53\n","Epoch : 391 \n","Generator Loss : 15.307375, Discriminator Loss : 0.296616, Time : 00:53\n","Epoch : 392 \n","Generator Loss : 16.175518, Discriminator Loss : 0.387963, Time : 00:53\n","Epoch : 393 \n","Generator Loss : 16.295404, Discriminator Loss : 0.445333, Time : 00:53\n","Epoch : 394 \n","Generator Loss : 17.370152, Discriminator Loss : 0.372158, Time : 00:53\n","Epoch : 395 \n","Generator Loss : 21.787195, Discriminator Loss : 0.240603, Time : 00:53\n","Epoch : 396 \n","Generator Loss : 19.611984, Discriminator Loss : 0.150680, Time : 00:53\n","Epoch : 397 \n","Generator Loss : 19.757908, Discriminator Loss : 0.245265, Time : 00:53\n","Epoch : 398 \n","Generator Loss : 17.064594, Discriminator Loss : 0.202348, Time : 00:53\n","Epoch : 399 \n","Generator Loss : 16.140232, Discriminator Loss : 0.110882, Time : 00:53\n","Epoch : 400 \n","Generator Loss : 16.402025, Discriminator Loss : 0.236636, Time : 00:53\n","400 model save complete\n","Epoch : 401 \n","Generator Loss : 17.809504, Discriminator Loss : 0.265928, Time : 00:56\n","Epoch : 402 \n","Generator Loss : 21.577568, Discriminator Loss : 0.423367, Time : 00:53\n","Epoch : 403 \n","Generator Loss : 17.080776, Discriminator Loss : 0.129556, Time : 00:53\n","Epoch : 404 \n","Generator Loss : 15.700947, Discriminator Loss : 0.402041, Time : 00:53\n","Epoch : 405 \n","Generator Loss : 14.269310, Discriminator Loss : 0.177966, Time : 00:53\n","Epoch : 406 \n","Generator Loss : 13.758940, Discriminator Loss : 0.284382, Time : 00:53\n","Epoch : 407 \n","Generator Loss : 25.327768, Discriminator Loss : 0.185507, Time : 00:53\n","Epoch : 408 \n","Generator Loss : 16.627916, Discriminator Loss : 0.085865, Time : 00:53\n","Epoch : 409 \n","Generator Loss : 16.316103, Discriminator Loss : 0.189580, Time : 00:53\n","Epoch : 410 \n","Generator Loss : 17.909340, Discriminator Loss : 0.253344, Time : 00:52\n","Epoch : 411 \n","Generator Loss : 21.052784, Discriminator Loss : 0.291572, Time : 00:53\n","Epoch : 412 \n","Generator Loss : 15.082039, Discriminator Loss : 0.327137, Time : 00:53\n","Epoch : 413 \n","Generator Loss : 17.146374, Discriminator Loss : 0.161058, Time : 00:53\n","Epoch : 414 \n","Generator Loss : 16.140936, Discriminator Loss : 0.176870, Time : 00:53\n","Epoch : 415 \n","Generator Loss : 19.151094, Discriminator Loss : 0.171556, Time : 00:53\n","Epoch : 416 \n","Generator Loss : 17.256338, Discriminator Loss : 0.153844, Time : 00:53\n","Epoch : 417 \n","Generator Loss : 15.631010, Discriminator Loss : 0.200428, Time : 00:53\n","Epoch : 418 \n","Generator Loss : 15.435015, Discriminator Loss : 0.255425, Time : 00:53\n","Epoch : 419 \n","Generator Loss : 17.496002, Discriminator Loss : 0.409260, Time : 00:53\n","Epoch : 420 \n","Generator Loss : 21.690756, Discriminator Loss : 0.158311, Time : 00:53\n","Epoch : 421 \n","Generator Loss : 15.186039, Discriminator Loss : 0.175601, Time : 00:53\n","Epoch : 422 \n","Generator Loss : 19.761877, Discriminator Loss : 0.099144, Time : 00:53\n","Epoch : 423 \n","Generator Loss : 22.513935, Discriminator Loss : 0.317396, Time : 00:53\n","Epoch : 424 \n","Generator Loss : 15.822739, Discriminator Loss : 0.199997, Time : 00:53\n","Epoch : 425 \n","Generator Loss : 18.306673, Discriminator Loss : 0.289280, Time : 00:53\n","Epoch : 426 \n","Generator Loss : 17.939161, Discriminator Loss : 0.339162, Time : 00:53\n","Epoch : 427 \n","Generator Loss : 16.688925, Discriminator Loss : 0.267007, Time : 00:53\n","Epoch : 428 \n","Generator Loss : 17.807888, Discriminator Loss : 0.301560, Time : 00:53\n","Epoch : 429 \n","Generator Loss : 15.275734, Discriminator Loss : 0.234058, Time : 00:53\n","Epoch : 430 \n","Generator Loss : 17.833622, Discriminator Loss : 0.361627, Time : 00:53\n","Epoch : 431 \n","Generator Loss : 19.411057, Discriminator Loss : 0.346961, Time : 00:53\n","Epoch : 432 \n","Generator Loss : 15.499280, Discriminator Loss : 0.358020, Time : 00:53\n","Epoch : 433 \n","Generator Loss : 18.887224, Discriminator Loss : 0.265180, Time : 00:53\n","Epoch : 434 \n","Generator Loss : 15.673948, Discriminator Loss : 0.320811, Time : 00:53\n","Epoch : 435 \n","Generator Loss : 20.145021, Discriminator Loss : 0.325495, Time : 00:53\n","Epoch : 436 \n","Generator Loss : 16.510031, Discriminator Loss : 0.210889, Time : 00:53\n","Epoch : 437 \n","Generator Loss : 16.283480, Discriminator Loss : 0.263927, Time : 00:53\n","Epoch : 438 \n","Generator Loss : 14.830704, Discriminator Loss : 0.182612, Time : 00:53\n","Epoch : 439 \n","Generator Loss : 13.910820, Discriminator Loss : 0.336625, Time : 00:53\n","Epoch : 440 \n","Generator Loss : 13.925788, Discriminator Loss : 0.536594, Time : 00:52\n","Epoch : 441 \n","Generator Loss : 16.532005, Discriminator Loss : 0.174475, Time : 00:52\n","Epoch : 442 \n","Generator Loss : 16.675774, Discriminator Loss : 0.247736, Time : 00:52\n","Epoch : 443 \n","Generator Loss : 15.464205, Discriminator Loss : 0.552924, Time : 00:53\n","Epoch : 444 \n","Generator Loss : 18.057053, Discriminator Loss : 0.370882, Time : 00:53\n","Epoch : 445 \n","Generator Loss : 14.141644, Discriminator Loss : 0.140245, Time : 00:53\n","Epoch : 446 \n","Generator Loss : 18.288872, Discriminator Loss : 0.158738, Time : 00:53\n","Epoch : 447 \n","Generator Loss : 14.951536, Discriminator Loss : 0.183976, Time : 00:53\n","Epoch : 448 \n","Generator Loss : 17.697468, Discriminator Loss : 0.169107, Time : 00:52\n","Epoch : 449 \n","Generator Loss : 19.975664, Discriminator Loss : 0.296744, Time : 00:52\n","Epoch : 450 \n","Generator Loss : 18.225040, Discriminator Loss : 0.436488, Time : 00:52\n","Epoch : 451 \n","Generator Loss : 17.757957, Discriminator Loss : 0.160790, Time : 00:52\n","Epoch : 452 \n","Generator Loss : 16.741550, Discriminator Loss : 0.062271, Time : 00:53\n","Epoch : 453 \n","Generator Loss : 15.554069, Discriminator Loss : 0.161411, Time : 00:53\n","Epoch : 454 \n","Generator Loss : 17.782749, Discriminator Loss : 0.130013, Time : 00:52\n","Epoch : 455 \n","Generator Loss : 15.966955, Discriminator Loss : 0.092787, Time : 00:53\n","Epoch : 456 \n","Generator Loss : 17.560259, Discriminator Loss : 0.088886, Time : 00:53\n","Epoch : 457 \n","Generator Loss : 16.415274, Discriminator Loss : 0.107869, Time : 00:53\n","Epoch : 458 \n","Generator Loss : 16.367821, Discriminator Loss : 0.133155, Time : 00:53\n","Epoch : 459 \n","Generator Loss : 15.363667, Discriminator Loss : 0.283166, Time : 00:52\n","Epoch : 460 \n","Generator Loss : 13.439184, Discriminator Loss : 0.256634, Time : 00:52\n","Epoch : 461 \n","Generator Loss : 19.217358, Discriminator Loss : 0.285553, Time : 00:53\n","Epoch : 462 \n","Generator Loss : 16.470892, Discriminator Loss : 0.280590, Time : 00:52\n","Epoch : 463 \n","Generator Loss : 17.350147, Discriminator Loss : 0.166480, Time : 00:52\n","Epoch : 464 \n","Generator Loss : 14.994497, Discriminator Loss : 0.157765, Time : 00:53\n","Epoch : 465 \n","Generator Loss : 15.010622, Discriminator Loss : 0.186367, Time : 00:53\n","Epoch : 466 \n","Generator Loss : 20.128126, Discriminator Loss : 0.309171, Time : 00:53\n","Epoch : 467 \n","Generator Loss : 15.699724, Discriminator Loss : 0.189706, Time : 00:53\n","Epoch : 468 \n","Generator Loss : 15.058080, Discriminator Loss : 0.208391, Time : 00:53\n","Epoch : 469 \n","Generator Loss : 18.196287, Discriminator Loss : 0.201417, Time : 00:53\n","Epoch : 470 \n","Generator Loss : 19.011707, Discriminator Loss : 0.148799, Time : 00:53\n","Epoch : 471 \n","Generator Loss : 14.355106, Discriminator Loss : 0.197026, Time : 00:53\n","Epoch : 472 \n","Generator Loss : 17.922125, Discriminator Loss : 0.106915, Time : 00:53\n","Epoch : 473 \n","Generator Loss : 18.203510, Discriminator Loss : 0.178280, Time : 00:53\n","Epoch : 474 \n","Generator Loss : 16.633173, Discriminator Loss : 0.098593, Time : 00:53\n","Epoch : 475 \n","Generator Loss : 16.545383, Discriminator Loss : 0.142506, Time : 00:53\n","Epoch : 476 \n","Generator Loss : 17.314192, Discriminator Loss : 0.156208, Time : 00:53\n","Epoch : 477 \n","Generator Loss : 16.701513, Discriminator Loss : 0.219872, Time : 00:53\n","Epoch : 478 \n","Generator Loss : 18.089027, Discriminator Loss : 0.227867, Time : 00:53\n","Epoch : 479 \n","Generator Loss : 17.711376, Discriminator Loss : 0.124127, Time : 00:53\n","Epoch : 480 \n","Generator Loss : 15.304358, Discriminator Loss : 0.305721, Time : 00:53\n","Epoch : 481 \n","Generator Loss : 18.592033, Discriminator Loss : 0.138263, Time : 00:53\n","Epoch : 482 \n","Generator Loss : 13.654671, Discriminator Loss : 0.168302, Time : 00:53\n","Epoch : 483 \n","Generator Loss : 18.976353, Discriminator Loss : 0.083787, Time : 00:53\n","Epoch : 484 \n","Generator Loss : 19.315092, Discriminator Loss : 0.189323, Time : 00:53\n","Epoch : 485 \n","Generator Loss : 16.563652, Discriminator Loss : 0.152487, Time : 00:53\n","Epoch : 486 \n","Generator Loss : 16.255615, Discriminator Loss : 0.314514, Time : 00:53\n","Epoch : 487 \n","Generator Loss : 16.373146, Discriminator Loss : 0.195575, Time : 00:52\n","Epoch : 488 \n","Generator Loss : 16.409321, Discriminator Loss : 0.249660, Time : 00:53\n","Epoch : 489 \n","Generator Loss : 20.357347, Discriminator Loss : 0.137587, Time : 00:53\n","Epoch : 490 \n","Generator Loss : 17.484703, Discriminator Loss : 0.350975, Time : 00:53\n","Epoch : 491 \n","Generator Loss : 16.520689, Discriminator Loss : 0.207044, Time : 00:53\n","Epoch : 492 \n","Generator Loss : 18.410353, Discriminator Loss : 0.508940, Time : 00:53\n","Epoch : 493 \n","Generator Loss : 12.534045, Discriminator Loss : 0.250969, Time : 00:53\n","Epoch : 494 \n","Generator Loss : 16.034422, Discriminator Loss : 0.267722, Time : 00:53\n","Epoch : 495 \n","Generator Loss : 15.287098, Discriminator Loss : 0.206004, Time : 00:53\n","Epoch : 496 \n","Generator Loss : 17.174011, Discriminator Loss : 0.391239, Time : 00:53\n","Epoch : 497 \n","Generator Loss : 16.156548, Discriminator Loss : 0.315105, Time : 00:53\n","Epoch : 498 \n","Generator Loss : 18.684654, Discriminator Loss : 0.314092, Time : 00:53\n","Epoch : 499 \n","Generator Loss : 13.655649, Discriminator Loss : 0.257007, Time : 00:53\n","Save model with incorrect termination or training ended\n","Training Done!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hbTa21tvCpn-"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"5VyHwMbQCqxv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605677763373,"user_tz":-180,"elapsed":161070,"user":{"displayName":"Aigul Sibgatullina","photoUrl":"","userId":"02965597584056384610"}},"outputId":"f2f86608-7cfb-4659-ae31-a4c38ac5788c"},"source":["!python test.py"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","2020-11-18 05:34:07.130113: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","============================== test ==============================\n","ckpt!!! model_checkpoint_path: \"./model/model-499\"\n","all_model_checkpoint_paths: \"./model/model-100\"\n","all_model_checkpoint_paths: \"./model/model-200\"\n","all_model_checkpoint_paths: \"./model/model-300\"\n","all_model_checkpoint_paths: \"./model/model-400\"\n","all_model_checkpoint_paths: \"./model/model-499\"\n","\n","Loading Previous Checkpoint ./model/model-499\n","  Global step was: 499\n","  Restoring... Model Load Done.\n","/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/Utils/utils.py:172: RuntimeWarning: divide by zero encountered in log\n","  f0_converted = np.exp((np.log(f0) - mean_log_src) / std_log_src * std_log_target + mean_log_target)\n","/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/Utils/utils.py:172: RuntimeWarning: divide by zero encountered in log\n","  f0_converted = np.exp((np.log(f0) - mean_log_src) / std_log_src * std_log_target + mean_log_target)\n","/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/Utils/utils.py:172: RuntimeWarning: divide by zero encountered in log\n","  f0_converted = np.exp((np.log(f0) - mean_log_src) / std_log_src * std_log_target + mean_log_target)\n","/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/Utils/utils.py:172: RuntimeWarning: divide by zero encountered in log\n","  f0_converted = np.exp((np.log(f0) - mean_log_src) / std_log_src * std_log_target + mean_log_target)\n","/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/Utils/utils.py:172: RuntimeWarning: divide by zero encountered in log\n","  f0_converted = np.exp((np.log(f0) - mean_log_src) / std_log_src * std_log_target + mean_log_target)\n","/content/drive/My Drive/Singing-Voice-Conversion-master/Vocal_Style_Transfer/Utils/utils.py:172: RuntimeWarning: divide by zero encountered in log\n","  f0_converted = np.exp((np.log(f0) - mean_log_src) / std_log_src * std_log_target + mean_log_target)\n","Test.py Sample Create Completed!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f_Rby2R5WbzX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Glb8pNhsct7k"},"source":[""]}]}